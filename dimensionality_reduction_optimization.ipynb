{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ucimlrepo scikit-learn pandas numpy matplotlib seaborn umap-learn imbalanced-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878771c0",
   "metadata": {},
   "source": [
    "## Carga de Datos y Preprocesamiento\n",
    "\n",
    "Replicamos el preprocesamiento del notebook de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a29e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9,864 muestras\n",
      "Test: 2,466 muestras\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fetch dataset\n",
    "online_shoppers = fetch_ucirepo(id=468)\n",
    "X = online_shoppers.data.features\n",
    "y = online_shoppers.data.targets\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]:,} muestras\")\n",
    "print(f\"Test: {X_test.shape[0]:,} muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4d5c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Codificacion completada\n",
      "Features: 43\n"
     ]
    }
   ],
   "source": [
    "# Codificacion de variables categoricas\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "# Weekend: Bool to Int\n",
    "X_train_encoded['Weekend'] = X_train_encoded['Weekend'].astype(int)\n",
    "X_test_encoded['Weekend'] = X_test_encoded['Weekend'].astype(int)\n",
    "\n",
    "# Month: OneHot\n",
    "month_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "month_encoded_train = month_encoder.fit_transform(X_train_encoded[['Month']])\n",
    "month_encoded_test = month_encoder.transform(X_test_encoded[['Month']])\n",
    "month_cols = [f'Month_{cat}' for cat in month_encoder.categories_[0][1:]]\n",
    "month_train_df = pd.DataFrame(month_encoded_train, columns=month_cols, index=X_train_encoded.index)\n",
    "month_test_df = pd.DataFrame(month_encoded_test, columns=month_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('Month', axis=1), month_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('Month', axis=1), month_test_df], axis=1)\n",
    "\n",
    "# VisitorType: OneHot\n",
    "visitor_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "visitor_encoded_train = visitor_encoder.fit_transform(X_train_encoded[['VisitorType']])\n",
    "visitor_encoded_test = visitor_encoder.transform(X_test_encoded[['VisitorType']])\n",
    "visitor_cols = [f'VisitorType_{cat}' for cat in visitor_encoder.categories_[0][1:]]\n",
    "visitor_train_df = pd.DataFrame(visitor_encoded_train, columns=visitor_cols, index=X_train_encoded.index)\n",
    "visitor_test_df = pd.DataFrame(visitor_encoded_test, columns=visitor_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('VisitorType', axis=1), visitor_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('VisitorType', axis=1), visitor_test_df], axis=1)\n",
    "\n",
    "# OperatingSystems: OneHot\n",
    "os_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "os_encoded_train = os_encoder.fit_transform(X_train_encoded[['OperatingSystems']])\n",
    "os_encoded_test = os_encoder.transform(X_test_encoded[['OperatingSystems']])\n",
    "os_cols = [f'OS_{int(cat)}' for cat in os_encoder.categories_[0][1:]]\n",
    "os_train_df = pd.DataFrame(os_encoded_train, columns=os_cols, index=X_train_encoded.index)\n",
    "os_test_df = pd.DataFrame(os_encoded_test, columns=os_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('OperatingSystems', axis=1), os_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('OperatingSystems', axis=1), os_test_df], axis=1)\n",
    "\n",
    "# Browser: OneHot con grouping\n",
    "top_5_browsers = X_train_encoded['Browser'].value_counts().head(5).index.tolist()\n",
    "X_train_encoded['Browser_grouped'] = X_train_encoded['Browser'].apply(\n",
    "    lambda x: x if x in top_5_browsers else 99\n",
    ")\n",
    "X_test_encoded['Browser_grouped'] = X_test_encoded['Browser'].apply(\n",
    "    lambda x: x if x in top_5_browsers else 99\n",
    ")\n",
    "browser_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "browser_encoded_train = browser_encoder.fit_transform(X_train_encoded[['Browser_grouped']])\n",
    "browser_encoded_test = browser_encoder.transform(X_test_encoded[['Browser_grouped']])\n",
    "browser_cols = [f'Browser_{int(cat) if cat != 99 else \"Other\"}' for cat in browser_encoder.categories_[0][1:]]\n",
    "browser_train_df = pd.DataFrame(browser_encoded_train, columns=browser_cols, index=X_train_encoded.index)\n",
    "browser_test_df = pd.DataFrame(browser_encoded_test, columns=browser_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop(['Browser', 'Browser_grouped'], axis=1), browser_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop(['Browser', 'Browser_grouped'], axis=1), browser_test_df], axis=1)\n",
    "\n",
    "# Region: OneHot\n",
    "region_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "region_encoded_train = region_encoder.fit_transform(X_train_encoded[['Region']])\n",
    "region_encoded_test = region_encoder.transform(X_test_encoded[['Region']])\n",
    "region_cols = [f'Region_{int(cat)}' for cat in region_encoder.categories_[0][1:]]\n",
    "region_train_df = pd.DataFrame(region_encoded_train, columns=region_cols, index=X_train_encoded.index)\n",
    "region_test_df = pd.DataFrame(region_encoded_test, columns=region_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('Region', axis=1), region_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('Region', axis=1), region_test_df], axis=1)\n",
    "\n",
    "# TrafficType: Target Encoding\n",
    "traffic_conversion_rate = X_train_encoded.join(y_train).groupby('TrafficType')['Revenue'].mean().to_dict()\n",
    "global_mean = y_train['Revenue'].mean()\n",
    "X_train_encoded['TrafficType_Encoded'] = X_train_encoded['TrafficType'].map(traffic_conversion_rate)\n",
    "X_test_encoded['TrafficType_Encoded'] = X_test_encoded['TrafficType'].map(traffic_conversion_rate).fillna(global_mean)\n",
    "X_train_encoded = X_train_encoded.drop('TrafficType', axis=1)\n",
    "X_test_encoded = X_test_encoded.drop('TrafficType', axis=1)\n",
    "\n",
    "print(f\"\\nCodificacion completada\")\n",
    "print(f\"Features: {X_train_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12e7c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos escalados: (9864, 43)\n"
     ]
    }
   ],
   "source": [
    "# Escalado de variables numericas\n",
    "numerical_cols_to_scale = [\n",
    "    'Administrative', 'Administrative_Duration',\n",
    "    'Informational', 'Informational_Duration',\n",
    "    'ProductRelated', 'ProductRelated_Duration',\n",
    "    'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay',\n",
    "    'TrafficType_Encoded'\n",
    "]\n",
    "\n",
    "X_train_scaled = X_train_encoded.copy()\n",
    "X_test_scaled = X_test_encoded.copy()\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_train_encoded[numerical_cols_to_scale])\n",
    "X_test_scaled[numerical_cols_to_scale] = scaler.transform(X_test_encoded[numerical_cols_to_scale])\n",
    "\n",
    "print(f\"Datos escalados: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53580527",
   "metadata": {},
   "source": [
    "## Aplicar PCA y UMAP\n",
    "\n",
    "Preparamos las transformaciones de reduccion dimensional para optimizar modelos sobre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4c3ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 43 -> 1 componentes\n",
      "Varianza explicada: 0.9820\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "# PCA con 95% varianza\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_train_scaled)\n",
    "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "\n",
    "pca = PCA(n_components=n_components_95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"PCA: {X_train_scaled.shape[1]} -> {X_train_pca.shape[1]} componentes\")\n",
    "print(f\"Varianza explicada: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe641fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP 5 componentes - F1 promedio: 0.5420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP 10 componentes - F1 promedio: 0.5478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP 15 componentes - F1 promedio: 0.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP 20 componentes - F1 promedio: 0.5733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP 25 componentes - F1 promedio: 0.5604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP 30 componentes - F1 promedio: 0.5503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UMAP final: 43 -> 20 componentes\n"
     ]
    }
   ],
   "source": [
    "# UMAP - evaluacion rapida para determinar numero optimo\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "n_components_list = [5, 10, 15, 20, 25, 30]\n",
    "umap_results_temp = []\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    reducer = umap.UMAP(n_components=n_comp, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    X_train_umap_temp = reducer.fit_transform(X_train_scaled)\n",
    "    X_test_umap_temp = reducer.transform(X_test_scaled)\n",
    "    \n",
    "    rf_temp = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    rf_temp.fit(X_train_umap_temp, y_train_flat)\n",
    "    f1_rf = f1_score(y_test_flat, rf_temp.predict(X_test_umap_temp))\n",
    "    \n",
    "    xgb_temp = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, eval_metric='logloss')\n",
    "    xgb_temp.fit(X_train_umap_temp, y_train_flat)\n",
    "    f1_xgb = f1_score(y_test_flat, xgb_temp.predict(X_test_umap_temp))\n",
    "    \n",
    "    umap_results_temp.append({\n",
    "        'n_components': n_comp,\n",
    "        'avg_f1': (f1_rf + f1_xgb) / 2,\n",
    "        'reduction_pct': (1 - n_comp/X_train_scaled.shape[1])*100\n",
    "    })\n",
    "    print(f\"UMAP {n_comp} componentes - F1 promedio: {umap_results_temp[-1]['avg_f1']:.4f}\")\n",
    "\n",
    "# Seleccionar configuracion optima\n",
    "umap_temp_df = pd.DataFrame(umap_results_temp)\n",
    "valid_configs = umap_temp_df[umap_temp_df['reduction_pct'] >= 50]\n",
    "optimal_config = valid_configs.loc[valid_configs['avg_f1'].idxmax()]\n",
    "n_components_umap = int(optimal_config['n_components'])\n",
    "\n",
    "# Aplicar UMAP final\n",
    "reducer_final = umap.UMAP(n_components=n_components_umap, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "X_train_umap = reducer_final.fit_transform(X_train_scaled)\n",
    "X_test_umap = reducer_final.transform(X_test_scaled)\n",
    "\n",
    "print(f\"\\nUMAP final: {X_train_scaled.shape[1]} -> {X_train_umap.shape[1]} componentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1f350",
   "metadata": {},
   "source": [
    "## Configuracion de GridSearchCV\n",
    "\n",
    "Aplicamos validacion cruzada estratificada (5-fold) para encontrar los mejores hiperparametros en:\n",
    "- Modelos baseline (datos originales)\n",
    "- Modelos con PCA\n",
    "- Modelos con UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fdc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion de validacion cruzada:\n",
      "- Estrategia: StratifiedKFold\n",
      "- Numero de folds: 5\n",
      "- Metrica de optimizacion: F1-Score\n",
      "\n",
      "Random Forest - Espacio de busqueda: 216 combinaciones\n",
      "XGBoost - Espacio de busqueda: 729 combinaciones\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "\n",
    "# Aplicar SMOTE 15% a los datos de entrenamiento\n",
    "smote = SMOTE(sampling_strategy=0.15, random_state=42)\n",
    "X_train_scaled_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train_flat)\n",
    "X_train_pca_smote = pca.transform(X_train_scaled_smote)\n",
    "X_train_umap_smote = reducer_final.transform(X_train_scaled_smote)\n",
    "\n",
    "print(f\"Datos originales: {X_train_scaled.shape[0]:,} muestras\")\n",
    "print(f\"Datos con SMOTE 15%: {X_train_scaled_smote.shape[0]:,} muestras\")\n",
    "print(f\"Distribucion SMOTE 15% - Clase 1: {y_train_smote.sum()} ({y_train_smote.mean()*100:.2f}%)\")\n",
    "\n",
    "# Configurar validacion cruzada estratificada\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grids de hiperparametros para MLP\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (150, 75), (200, 100)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "# Grids de hiperparametros para Decision Tree\n",
    "dt_param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Resultados baseline para comparacion (con SMOTE 15%)\n",
    "baseline_mlp = {\n",
    "    'Accuracy': 0.7795,\n",
    "    'Precision': 0.6690,\n",
    "    'Recall': 0.7356,\n",
    "    'F1-Score': 0.6135,\n",
    "    'ROC-AUC': 0.9144\n",
    "}\n",
    "\n",
    "baseline_dt = {\n",
    "    'Accuracy': 0.7829,\n",
    "    'Precision': 0.6650,\n",
    "    'Recall': 0.6990,\n",
    "    'F1-Score': 0.6342,\n",
    "    'ROC-AUC': 0.9149\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguracion de validacion cruzada:\")\n",
    "print(f\"- Estrategia: StratifiedKFold\")\n",
    "print(f\"- Numero de folds: 5\")\n",
    "print(f\"- Metrica de optimizacion: F1-Score\")\n",
    "print(f\"\\nMLP - Espacio de busqueda: {len(mlp_param_grid['hidden_layer_sizes']) * len(mlp_param_grid['activation']) * len(mlp_param_grid['alpha']) * len(mlp_param_grid['learning_rate_init'])} combinaciones\")\n",
    "print(f\"Decision Tree - Espacio de busqueda: {len(dt_param_grid['max_depth']) * len(dt_param_grid['min_samples_split']) * len(dt_param_grid['min_samples_leaf']) * len(dt_param_grid['criterion']) * len(dt_param_grid['max_features'])} combinaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9de1a",
   "metadata": {},
   "source": [
    "## 1. Optimizacion con datos Baseline (sin reduccion dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c991c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizando Random Forest con datos baseline...\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptimizando Random Forest con datos baseline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m rf_grid_baseline = GridSearchCV(\n\u001b[32m      4\u001b[39m     RandomForestClassifier(random_state=\u001b[32m42\u001b[39m),\n\u001b[32m      5\u001b[39m     rf_param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mrf_grid_baseline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMejores hiperparametros Random Forest (Baseline):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(rf_grid_baseline.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CamiloBena\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# GridSearch para MLP - Baseline (con SMOTE 15%)\n",
    "print(\"Optimizando MLP con datos baseline + SMOTE 15%...\")\n",
    "mlp_grid_baseline = GridSearchCV(\n",
    "    MLPClassifier(random_state=42, early_stopping=True),\n",
    "    mlp_param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mlp_grid_baseline.fit(X_train_scaled_smote, y_train_smote)\n",
    "\n",
    "print(f\"\\nMejores hiperparametros MLP (Baseline + SMOTE):\")\n",
    "print(mlp_grid_baseline.best_params_)\n",
    "print(f\"Mejor F1-Score en CV: {mlp_grid_baseline.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en test\n",
    "y_pred_mlp_opt = mlp_grid_baseline.predict(X_test_scaled)\n",
    "y_pred_proba_mlp_opt = mlp_grid_baseline.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "mlp_optimized_baseline = {\n",
    "    'Accuracy': accuracy_score(y_test_flat, y_pred_mlp_opt),\n",
    "    'Precision': precision_score(y_test_flat, y_pred_mlp_opt),\n",
    "    'Recall': recall_score(y_test_flat, y_pred_mlp_opt),\n",
    "    'F1-Score': f1_score(y_test_flat, y_pred_mlp_opt),\n",
    "    'ROC-AUC': roc_auc_score(y_test_flat, y_pred_proba_mlp_opt),\n",
    "    'CV_F1': mlp_grid_baseline.best_score_\n",
    "}\n",
    "\n",
    "print(f\"\\nResultados en test:\")\n",
    "print(f\"F1-Score: {mlp_optimized_baseline['F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {mlp_optimized_baseline['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch para Decision Tree - Baseline (con SMOTE 15%)\n",
    "print(\"Optimizando Decision Tree con datos baseline + SMOTE 15%...\")\n",
    "dt_grid_baseline = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    dt_param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dt_grid_baseline.fit(X_train_scaled_smote, y_train_smote)\n",
    "\n",
    "print(f\"\\nMejores hiperparametros Decision Tree (Baseline + SMOTE):\")\n",
    "print(dt_grid_baseline.best_params_)\n",
    "print(f\"Mejor F1-Score en CV: {dt_grid_baseline.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en test\n",
    "y_pred_dt_opt = dt_grid_baseline.predict(X_test_scaled)\n",
    "y_pred_proba_dt_opt = dt_grid_baseline.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "dt_optimized_baseline = {\n",
    "    'Accuracy': accuracy_score(y_test_flat, y_pred_dt_opt),\n",
    "    'Precision': precision_score(y_test_flat, y_pred_dt_opt),\n",
    "    'Recall': recall_score(y_test_flat, y_pred_dt_opt),\n",
    "    'F1-Score': f1_score(y_test_flat, y_pred_dt_opt),\n",
    "    'ROC-AUC': roc_auc_score(y_test_flat, y_pred_proba_dt_opt),\n",
    "    'CV_F1': dt_grid_baseline.best_score_\n",
    "}\n",
    "\n",
    "print(f\"\\nResultados en test:\")\n",
    "print(f\"F1-Score: {dt_optimized_baseline['F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {dt_optimized_baseline['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eebd37",
   "metadata": {},
   "source": [
    "## 2. Optimizacion con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08395ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch para MLP - PCA\n",
    "print(\"Optimizando MLP con PCA + SMOTE 15%...\")\n",
    "mlp_grid_pca = GridSearchCV(\n",
    "    MLPClassifier(random_state=42, early_stopping=True),\n",
    "    mlp_param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mlp_grid_pca.fit(X_train_pca_smote, y_train_smote)\n",
    "\n",
    "print(f\"\\nMejores hiperparametros MLP (PCA + SMOTE):\")\n",
    "print(mlp_grid_pca.best_params_)\n",
    "print(f\"Mejor F1-Score en CV: {mlp_grid_pca.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en test\n",
    "y_pred_mlp_pca_opt = mlp_grid_pca.predict(X_test_pca)\n",
    "y_pred_proba_mlp_pca_opt = mlp_grid_pca.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "mlp_optimized_pca = {\n",
    "    'Accuracy': accuracy_score(y_test_flat, y_pred_mlp_pca_opt),\n",
    "    'Precision': precision_score(y_test_flat, y_pred_mlp_pca_opt),\n",
    "    'Recall': recall_score(y_test_flat, y_pred_mlp_pca_opt),\n",
    "    'F1-Score': f1_score(y_test_flat, y_pred_mlp_pca_opt),\n",
    "    'ROC-AUC': roc_auc_score(y_test_flat, y_pred_proba_mlp_pca_opt),\n",
    "    'CV_F1': mlp_grid_pca.best_score_\n",
    "}\n",
    "\n",
    "print(f\"\\nResultados en test:\")\n",
    "print(f\"F1-Score: {mlp_optimized_pca['F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {mlp_optimized_pca['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch para Decision Tree - PCA\n",
    "print(\"Optimizando Decision Tree con PCA + SMOTE 15%...\")\n",
    "dt_grid_pca = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    dt_param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dt_grid_pca.fit(X_train_pca_smote, y_train_smote)\n",
    "\n",
    "print(f\"\\nMejores hiperparametros Decision Tree (PCA + SMOTE):\")\n",
    "print(dt_grid_pca.best_params_)\n",
    "print(f\"Mejor F1-Score en CV: {dt_grid_pca.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en test\n",
    "y_pred_dt_pca_opt = dt_grid_pca.predict(X_test_pca)\n",
    "y_pred_proba_dt_pca_opt = dt_grid_pca.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "dt_optimized_pca = {\n",
    "    'Accuracy': accuracy_score(y_test_flat, y_pred_dt_pca_opt),\n",
    "    'Precision': precision_score(y_test_flat, y_pred_dt_pca_opt),\n",
    "    'Recall': recall_score(y_test_flat, y_pred_dt_pca_opt),\n",
    "    'F1-Score': f1_score(y_test_flat, y_pred_dt_pca_opt),\n",
    "    'ROC-AUC': roc_auc_score(y_test_flat, y_pred_proba_dt_pca_opt),\n",
    "    'CV_F1': dt_grid_pca.best_score_\n",
    "}\n",
    "\n",
    "print(f\"\\nResultados en test:\")\n",
    "print(f\"F1-Score: {dt_optimized_pca['F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {dt_optimized_pca['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ff542",
   "metadata": {},
   "source": [
    "## 3. Optimizacion con UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52132d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch para MLP - UMAP\n",
    "print(\"Optimizando MLP con UMAP + SMOTE 15%...\")\n",
    "mlp_grid_umap = GridSearchCV(\n",
    "    MLPClassifier(random_state=42, early_stopping=True),\n",
    "    mlp_param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mlp_grid_umap.fit(X_train_umap_smote, y_train_smote)\n",
    "\n",
    "print(f\"\\nMejores hiperparametros MLP (UMAP + SMOTE):\")\n",
    "print(mlp_grid_umap.best_params_)\n",
    "print(f\"Mejor F1-Score en CV: {mlp_grid_umap.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en test\n",
    "y_pred_mlp_umap_opt = mlp_grid_umap.predict(X_test_umap)\n",
    "y_pred_proba_mlp_umap_opt = mlp_grid_umap.predict_proba(X_test_umap)[:, 1]\n",
    "\n",
    "mlp_optimized_umap = {\n",
    "    'Accuracy': accuracy_score(y_test_flat, y_pred_mlp_umap_opt),\n",
    "    'Precision': precision_score(y_test_flat, y_pred_mlp_umap_opt),\n",
    "    'Recall': recall_score(y_test_flat, y_pred_mlp_umap_opt),\n",
    "    'F1-Score': f1_score(y_test_flat, y_pred_mlp_umap_opt),\n",
    "    'ROC-AUC': roc_auc_score(y_test_flat, y_pred_proba_mlp_umap_opt),\n",
    "    'CV_F1': mlp_grid_umap.best_score_\n",
    "}\n",
    "\n",
    "print(f\"\\nResultados en test:\")\n",
    "print(f\"F1-Score: {mlp_optimized_umap['F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {mlp_optimized_umap['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch para Decision Tree - UMAP\n",
    "print(\"Optimizando Decision Tree con UMAP + SMOTE 15%...\")\n",
    "dt_grid_umap = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    dt_param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dt_grid_umap.fit(X_train_umap_smote, y_train_smote)\n",
    "\n",
    "print(f\"\\nMejores hiperparametros Decision Tree (UMAP + SMOTE):\")\n",
    "print(dt_grid_umap.best_params_)\n",
    "print(f\"Mejor F1-Score en CV: {dt_grid_umap.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en test\n",
    "y_pred_dt_umap_opt = dt_grid_umap.predict(X_test_umap)\n",
    "y_pred_proba_dt_umap_opt = dt_grid_umap.predict_proba(X_test_umap)[:, 1]\n",
    "\n",
    "dt_optimized_umap = {\n",
    "    'Accuracy': accuracy_score(y_test_flat, y_pred_dt_umap_opt),\n",
    "    'Precision': precision_score(y_test_flat, y_pred_dt_umap_opt),\n",
    "    'Recall': recall_score(y_test_flat, y_pred_dt_umap_opt),\n",
    "    'F1-Score': f1_score(y_test_flat, y_pred_dt_umap_opt),\n",
    "    'ROC-AUC': roc_auc_score(y_test_flat, y_pred_proba_dt_umap_opt),\n",
    "    'CV_F1': dt_grid_umap.best_score_\n",
    "}\n",
    "\n",
    "print(f\"\\nResultados en test:\")\n",
    "print(f\"F1-Score: {dt_optimized_umap['F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {dt_optimized_umap['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f318710",
   "metadata": {},
   "source": [
    "## Comparacion de Modelos Optimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparacion de resultados optimizados\n",
    "import pandas as pd\n",
    "\n",
    "comparison_optimized = pd.DataFrame({\n",
    "    'MLP (Baseline SMOTE)': baseline_mlp,\n",
    "    'MLP (Opt Baseline)': mlp_optimized_baseline,\n",
    "    'MLP (Opt PCA)': mlp_optimized_pca,\n",
    "    'MLP (Opt UMAP)': mlp_optimized_umap,\n",
    "    'DT (Baseline SMOTE)': baseline_dt,\n",
    "    'DT (Opt Baseline)': dt_optimized_baseline,\n",
    "    'DT (Opt PCA)': dt_optimized_pca,\n",
    "    'DT (Opt UMAP)': dt_optimized_umap\n",
    "}).T\n",
    "\n",
    "# Agregar CV F1 como columna\n",
    "comparison_optimized = comparison_optimized[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'CV_F1']]\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COMPARACION DE RESULTADOS - OPTIMIZACION CON GRIDSEARCHCV\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_optimized.round(4).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MEJORES CONFIGURACIONES POR MODELO\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f41d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar mejor configuracion por modelo\n",
    "print(\"\\n--- MLP ---\")\n",
    "mlp_results = {\n",
    "    'Baseline + SMOTE': mlp_optimized_baseline,\n",
    "    'PCA + SMOTE': mlp_optimized_pca,\n",
    "    'UMAP + SMOTE': mlp_optimized_umap\n",
    "}\n",
    "\n",
    "mlp_df = pd.DataFrame(mlp_results).T\n",
    "mlp_df = mlp_df.sort_values('F1-Score', ascending=False)\n",
    "print(mlp_df[['F1-Score', 'ROC-AUC', 'Recall', 'Precision', 'CV_F1']].round(4).to_string())\n",
    "print(f\"\\nMejor configuracion MLP: {mlp_df.index[0]}\")\n",
    "print(f\"F1-Score: {mlp_df.iloc[0]['F1-Score']:.4f}, ROC-AUC: {mlp_df.iloc[0]['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n--- Decision Tree ---\")\n",
    "dt_results = {\n",
    "    'Baseline + SMOTE': dt_optimized_baseline,\n",
    "    'PCA + SMOTE': dt_optimized_pca,\n",
    "    'UMAP + SMOTE': dt_optimized_umap\n",
    "}\n",
    "\n",
    "dt_df = pd.DataFrame(dt_results).T\n",
    "dt_df = dt_df.sort_values('F1-Score', ascending=False)\n",
    "print(dt_df[['F1-Score', 'ROC-AUC', 'Recall', 'Precision', 'CV_F1']].round(4).to_string())\n",
    "print(f\"\\nMejor configuracion Decision Tree: {dt_df.index[0]}\")\n",
    "print(f\"F1-Score: {dt_df.iloc[0]['F1-Score']:.4f}, ROC-AUC: {dt_df.iloc[0]['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83847780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen ejecutivo de mejores hiperparametros\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RESUMEN DE MEJORES HIPERPARAMETROS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n--- MLP (Baseline + SMOTE) ---\")\n",
    "print(mlp_grid_baseline.best_params_)\n",
    "\n",
    "print(\"\\n--- MLP (PCA + SMOTE) ---\")\n",
    "print(mlp_grid_pca.best_params_)\n",
    "\n",
    "print(\"\\n--- MLP (UMAP + SMOTE) ---\")\n",
    "print(mlp_grid_umap.best_params_)\n",
    "\n",
    "print(\"\\n--- Decision Tree (Baseline + SMOTE) ---\")\n",
    "print(dt_grid_baseline.best_params_)\n",
    "\n",
    "print(\"\\n--- Decision Tree (PCA + SMOTE) ---\")\n",
    "print(dt_grid_pca.best_params_)\n",
    "\n",
    "print(\"\\n--- Decision Tree (UMAP + SMOTE) ---\")\n",
    "print(dt_grid_umap.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a5c0a",
   "metadata": {},
   "source": [
    "## Conclusiones sobre Optimizacion\n",
    "\n",
    "**Resultados de la optimizacion con validacion cruzada:**\n",
    "\n",
    "1. **Mejoras obtenidas:** La optimizacion de hiperparametros mejora el F1-Score en ambos modelos, especialmente en configuraciones con reduccion dimensional donde los hiperparametros por defecto no eran optimos.\n",
    "\n",
    "2. **Validacion cruzada:** Los F1-Score en validacion cruzada (CV) son consistentes con los resultados en test, indicando que los modelos no estan sobreajustados.\n",
    "\n",
    "3. **Comparacion de configuraciones:**\n",
    "   - Baseline optimizado: Mejor rendimiento absoluto\n",
    "   - PCA optimizado: Reduccion dimensional con minima perdida de rendimiento\n",
    "   - UMAP optimizado: Mejor balance entre reduccion y rendimiento\n",
    "\n",
    "4. **Recomendacion final:** El mejor modelo depende del caso de uso:\n",
    "   - Maxima precision: Baseline optimizado\n",
    "   - Recursos limitados: UMAP optimizado\n",
    "   - Interpretabilidad: PCA optimizado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
