{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ucimlrepo scikit-learn xgboost pandas numpy matplotlib seaborn umap-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0bf79e",
   "metadata": {},
   "source": [
    "## Carga de Datos y Preprocesamiento\n",
    "\n",
    "Replicamos el preprocesamiento del notebook de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a26717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fetch dataset\n",
    "online_shoppers = fetch_ucirepo(id=468)\n",
    "X = online_shoppers.data.features\n",
    "y = online_shoppers.data.targets\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]:,} muestras\")\n",
    "print(f\"Test: {X_test.shape[0]:,} muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificacion de variables categoricas\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "# Weekend: Bool to Int\n",
    "X_train_encoded['Weekend'] = X_train_encoded['Weekend'].astype(int)\n",
    "X_test_encoded['Weekend'] = X_test_encoded['Weekend'].astype(int)\n",
    "\n",
    "# Month: OneHot\n",
    "month_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "month_encoded_train = month_encoder.fit_transform(X_train_encoded[['Month']])\n",
    "month_encoded_test = month_encoder.transform(X_test_encoded[['Month']])\n",
    "month_cols = [f'Month_{cat}' for cat in month_encoder.categories_[0][1:]]\n",
    "month_train_df = pd.DataFrame(month_encoded_train, columns=month_cols, index=X_train_encoded.index)\n",
    "month_test_df = pd.DataFrame(month_encoded_test, columns=month_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('Month', axis=1), month_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('Month', axis=1), month_test_df], axis=1)\n",
    "\n",
    "# VisitorType: OneHot\n",
    "visitor_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "visitor_encoded_train = visitor_encoder.fit_transform(X_train_encoded[['VisitorType']])\n",
    "visitor_encoded_test = visitor_encoder.transform(X_test_encoded[['VisitorType']])\n",
    "visitor_cols = [f'VisitorType_{cat}' for cat in visitor_encoder.categories_[0][1:]]\n",
    "visitor_train_df = pd.DataFrame(visitor_encoded_train, columns=visitor_cols, index=X_train_encoded.index)\n",
    "visitor_test_df = pd.DataFrame(visitor_encoded_test, columns=visitor_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('VisitorType', axis=1), visitor_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('VisitorType', axis=1), visitor_test_df], axis=1)\n",
    "\n",
    "# OperatingSystems: OneHot\n",
    "os_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "os_encoded_train = os_encoder.fit_transform(X_train_encoded[['OperatingSystems']])\n",
    "os_encoded_test = os_encoder.transform(X_test_encoded[['OperatingSystems']])\n",
    "os_cols = [f'OS_{int(cat)}' for cat in os_encoder.categories_[0][1:]]\n",
    "os_train_df = pd.DataFrame(os_encoded_train, columns=os_cols, index=X_train_encoded.index)\n",
    "os_test_df = pd.DataFrame(os_encoded_test, columns=os_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('OperatingSystems', axis=1), os_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('OperatingSystems', axis=1), os_test_df], axis=1)\n",
    "\n",
    "# Browser: OneHot con grouping\n",
    "top_5_browsers = X_train_encoded['Browser'].value_counts().head(5).index.tolist()\n",
    "X_train_encoded['Browser_grouped'] = X_train_encoded['Browser'].apply(\n",
    "    lambda x: x if x in top_5_browsers else 99\n",
    ")\n",
    "X_test_encoded['Browser_grouped'] = X_test_encoded['Browser'].apply(\n",
    "    lambda x: x if x in top_5_browsers else 99\n",
    ")\n",
    "browser_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "browser_encoded_train = browser_encoder.fit_transform(X_train_encoded[['Browser_grouped']])\n",
    "browser_encoded_test = browser_encoder.transform(X_test_encoded[['Browser_grouped']])\n",
    "browser_cols = [f'Browser_{int(cat) if cat != 99 else \"Other\"}' for cat in browser_encoder.categories_[0][1:]]\n",
    "browser_train_df = pd.DataFrame(browser_encoded_train, columns=browser_cols, index=X_train_encoded.index)\n",
    "browser_test_df = pd.DataFrame(browser_encoded_test, columns=browser_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop(['Browser', 'Browser_grouped'], axis=1), browser_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop(['Browser', 'Browser_grouped'], axis=1), browser_test_df], axis=1)\n",
    "\n",
    "# Region: OneHot\n",
    "region_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "region_encoded_train = region_encoder.fit_transform(X_train_encoded[['Region']])\n",
    "region_encoded_test = region_encoder.transform(X_test_encoded[['Region']])\n",
    "region_cols = [f'Region_{int(cat)}' for cat in region_encoder.categories_[0][1:]]\n",
    "region_train_df = pd.DataFrame(region_encoded_train, columns=region_cols, index=X_train_encoded.index)\n",
    "region_test_df = pd.DataFrame(region_encoded_test, columns=region_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('Region', axis=1), region_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('Region', axis=1), region_test_df], axis=1)\n",
    "\n",
    "# TrafficType: Target Encoding\n",
    "traffic_conversion_rate = X_train_encoded.join(y_train).groupby('TrafficType')['Revenue'].mean().to_dict()\n",
    "global_mean = y_train['Revenue'].mean()\n",
    "X_train_encoded['TrafficType_Encoded'] = X_train_encoded['TrafficType'].map(traffic_conversion_rate)\n",
    "X_test_encoded['TrafficType_Encoded'] = X_test_encoded['TrafficType'].map(traffic_conversion_rate).fillna(global_mean)\n",
    "X_train_encoded = X_train_encoded.drop('TrafficType', axis=1)\n",
    "X_test_encoded = X_test_encoded.drop('TrafficType', axis=1)\n",
    "\n",
    "print(f\"\\nCodificacion completada\")\n",
    "print(f\"Features: {X_train_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5440e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de variables numericas\n",
    "numerical_cols_to_scale = [\n",
    "    'Administrative', 'Administrative_Duration',\n",
    "    'Informational', 'Informational_Duration',\n",
    "    'ProductRelated', 'ProductRelated_Duration',\n",
    "    'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay',\n",
    "    'TrafficType_Encoded'\n",
    "]\n",
    "\n",
    "X_train_scaled = X_train_encoded.copy()\n",
    "X_test_scaled = X_test_encoded.copy()\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_train_encoded[numerical_cols_to_scale])\n",
    "X_test_scaled[numerical_cols_to_scale] = scaler.transform(X_test_encoded[numerical_cols_to_scale])\n",
    "\n",
    "print(f\"Datos escalados: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465a0a4",
   "metadata": {},
   "source": [
    "## 5.1. Analisis Individual de Variables\n",
    "\n",
    "### Paso 9: Correlacion e Indices de Discriminacion\n",
    "\n",
    "Evaluamos cada caracteristica usando:\n",
    "1. Correlacion de Pearson con la variable objetivo\n",
    "2. Mutual Information (capacidad discriminativa no lineal)\n",
    "3. Chi-cuadrado para variables categoricas\n",
    "4. Point-Biserial Correlation para variables continuas vs binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a53651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, chi2, f_classif\n",
    "from scipy.stats import pointbiserialr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Mutual Information\n",
    "mi_scores = mutual_info_classif(X_train_scaled, y_train_flat, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': X_train_scaled.columns,\n",
    "    'MI_Score': mi_scores\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(\"Top 10 caracteristicas por Mutual Information:\")\n",
    "print(mi_df.head(10).to_string(index=False))\n",
    "print(f\"\\nBottom 10 caracteristicas por Mutual Information:\")\n",
    "print(mi_df.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point-Biserial Correlation para variables numericas\n",
    "numerical_features = X_train_scaled[numerical_cols_to_scale].columns\n",
    "pb_correlations = []\n",
    "\n",
    "for col in numerical_features:\n",
    "    corr, pval = pointbiserialr(y_train_flat, X_train_scaled[col])\n",
    "    pb_correlations.append({\n",
    "        'Feature': col,\n",
    "        'PB_Correlation': abs(corr),\n",
    "        'P_Value': pval\n",
    "    })\n",
    "\n",
    "pb_df = pd.DataFrame(pb_correlations).sort_values('PB_Correlation', ascending=False)\n",
    "print(\"\\nPoint-Biserial Correlation (variables numericas):\")\n",
    "print(pb_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90827f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA F-statistic para todas las variables\n",
    "f_scores, f_pvalues = f_classif(X_train_scaled, y_train_flat)\n",
    "f_df = pd.DataFrame({\n",
    "    'Feature': X_train_scaled.columns,\n",
    "    'F_Score': f_scores,\n",
    "    'P_Value': f_pvalues\n",
    "}).sort_values('F_Score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 caracteristicas por ANOVA F-Score:\")\n",
    "print(f_df.head(15).to_string(index=False))\n",
    "print(f\"\\nBottom 10 caracteristicas por ANOVA F-Score:\")\n",
    "print(f_df.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f88eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizacion de importancia de caracteristicas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Mutual Information\n",
    "mi_df_top20 = mi_df.head(20)\n",
    "axes[0].barh(range(len(mi_df_top20)), mi_df_top20['MI_Score'])\n",
    "axes[0].set_yticks(range(len(mi_df_top20)))\n",
    "axes[0].set_yticklabels(mi_df_top20['Feature'])\n",
    "axes[0].set_xlabel('Mutual Information Score')\n",
    "axes[0].set_title('Top 20 Features - Mutual Information')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# F-Score\n",
    "f_df_top20 = f_df.head(20)\n",
    "axes[1].barh(range(len(f_df_top20)), f_df_top20['F_Score'])\n",
    "axes[1].set_yticks(range(len(f_df_top20)))\n",
    "axes[1].set_yticklabels(f_df_top20['Feature'])\n",
    "axes[1].set_xlabel('ANOVA F-Score')\n",
    "axes[1].set_title('Top 20 Features - ANOVA F-Score')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar caracteristicas candidatas a eliminacion\n",
    "# Criterio: MI_Score < 0.001 Y F_Score < 1.0 Y P_Value > 0.05\n",
    "\n",
    "weak_features = set()\n",
    "\n",
    "# Features con MI muy bajo\n",
    "weak_mi = set(mi_df[mi_df['MI_Score'] < 0.001]['Feature'])\n",
    "# Features con F-Score bajo y p-value alto\n",
    "weak_f = set(f_df[(f_df['F_Score'] < 1.0) & (f_df['P_Value'] > 0.05)]['Feature'])\n",
    "\n",
    "# Interseccion\n",
    "weak_features = weak_mi.intersection(weak_f)\n",
    "\n",
    "print(f\"\\nCaracteristicas candidatas a eliminacion (criterio conservador):\")\n",
    "print(f\"Total: {len(weak_features)}\")\n",
    "if len(weak_features) > 0:\n",
    "    for feat in sorted(weak_features):\n",
    "        mi_val = mi_df[mi_df['Feature'] == feat]['MI_Score'].values[0]\n",
    "        f_val = f_df[f_df['Feature'] == feat]['F_Score'].values[0]\n",
    "        p_val = f_df[f_df['Feature'] == feat]['P_Value'].values[0]\n",
    "        print(f\"  - {feat}: MI={mi_val:.4f}, F={f_val:.4f}, p={p_val:.4f}\")\n",
    "else:\n",
    "    print(\"  No se encontraron caracteristicas con discriminacion muy baja.\")\n",
    "    print(\"  Se usara seleccion por umbral en PCA/UMAP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2890b11",
   "metadata": {},
   "source": [
    "### Resumen Analisis Individual\n",
    "\n",
    "**Caracteristicas mas discriminativas (Top 5):**\n",
    "- PageValues: Valor monetario de la pagina visitada\n",
    "- ExitRates: Tasa de salida de la pagina\n",
    "- ProductRelated_Duration: Tiempo en paginas de productos\n",
    "- BounceRates: Tasa de rebote\n",
    "- Month_Nov: Mes de noviembre (temporada alta)\n",
    "\n",
    "**Caracteristicas candidatas a eliminacion:**\n",
    "Variables con bajo poder discriminativo identificadas anteriormente.\n",
    "\n",
    "**Conclusion:**\n",
    "Las variables relacionadas con el comportamiento del usuario (duracion, tasas de salida/rebote, valor de pagina) son las mas importantes. Variables categoricas de bajo nivel (algunos meses, regiones, browsers) tienen menor capacidad predictiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c60744",
   "metadata": {},
   "source": [
    "## 5.2. Extraccion de Caracteristicas Lineal (PCA)\n",
    "\n",
    "### Paso 10: Reduccion dimensional con PCA\n",
    "\n",
    "**Criterio de seleccion:** Mantener componentes que expliquen al menos el 95% de la varianza acumulada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95110aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA con todas las componentes para analizar varianza\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_train_scaled)\n",
    "\n",
    "# Varianza explicada acumulada\n",
    "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Grafico de varianza explicada\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Varianza individual\n",
    "axes[0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "            pca_full.explained_variance_ratio_)\n",
    "axes[0].set_xlabel('Componente Principal')\n",
    "axes[0].set_ylabel('Varianza Explicada')\n",
    "axes[0].set_title('Varianza Explicada por Componente')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Varianza acumulada\n",
    "axes[1].plot(range(1, len(cumsum_variance) + 1), cumsum_variance, marker='o')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% varianza')\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90% varianza')\n",
    "axes[1].set_xlabel('Numero de Componentes')\n",
    "axes[1].set_ylabel('Varianza Explicada Acumulada')\n",
    "axes[1].set_title('Varianza Acumulada')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determinar numero de componentes para 95% varianza\n",
    "n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "n_components_90 = np.argmax(cumsum_variance >= 0.90) + 1\n",
    "\n",
    "print(f\"\\nNumero de componentes originales: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Componentes para 90% varianza: {n_components_90}\")\n",
    "print(f\"Componentes para 95% varianza: {n_components_95}\")\n",
    "print(f\"\\nReduccion dimensional (95% varianza): {(1 - n_components_95/X_train_scaled.shape[1])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA con numero optimo de componentes\n",
    "pca = PCA(n_components=n_components_95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Dimensiones originales: {X_train_scaled.shape}\")\n",
    "print(f\"Dimensiones PCA: {X_train_pca.shape}\")\n",
    "print(f\"Varianza explicada total: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ca44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis de componentes principales\n",
    "# Top features por componente\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=X_train_scaled.columns,\n",
    "    index=[f'PC{i+1}' for i in range(n_components_95)]\n",
    ")\n",
    "\n",
    "print(\"Top 5 features con mayor peso absoluto en cada una de las primeras 5 componentes:\")\n",
    "for i in range(min(5, n_components_95)):\n",
    "    pc = f'PC{i+1}'\n",
    "    top_features = components_df.loc[pc].abs().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\n{pc} (Varianza explicada: {pca.explained_variance_ratio_[i]:.4f}):\")\n",
    "    for feat, weight in top_features.items():\n",
    "        original_weight = components_df.loc[pc, feat]\n",
    "        print(f\"  {feat}: {original_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc827a2",
   "metadata": {},
   "source": [
    "### Evaluacion de Modelos con PCA\n",
    "\n",
    "Evaluamos Random Forest y XGBoost (los 2 mejores modelos del entrenamiento) con las componentes PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import time\n",
    "\n",
    "# Modelos baseline (resultados originales)\n",
    "baseline_rf = {\n",
    "    'Accuracy': 0.8966,\n",
    "    'Precision': 0.7396,\n",
    "    'Recall': 0.5131,\n",
    "    'F1-Score': 0.6059,\n",
    "    'ROC-AUC': 0.9187\n",
    "}\n",
    "\n",
    "baseline_xgb = {\n",
    "    'Accuracy': 0.9030,\n",
    "    'Precision': 0.7213,\n",
    "    'Recall': 0.6099,\n",
    "    'F1-Score': 0.6609,\n",
    "    'ROC-AUC': 0.9283\n",
    "}\n",
    "\n",
    "# Entrenar modelos con PCA\n",
    "models_pca = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                             random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "pca_results = {}\n",
    "\n",
    "for name, model in models_pca.items():\n",
    "    print(f\"\\nEntrenando {name} con PCA...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(X_train_pca, y_train_flat)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    y_pred_proba = model.predict_proba(X_test_pca)[:, 1]\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    pca_results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test_flat, y_pred),\n",
    "        'Precision': precision_score(y_test_flat, y_pred),\n",
    "        'Recall': recall_score(y_test_flat, y_pred),\n",
    "        'F1-Score': f1_score(y_test_flat, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test_flat, y_pred_proba),\n",
    "        'Training Time (s)': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Completado en {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa211bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparacion de resultados PCA vs Baseline\n",
    "comparison_pca = pd.DataFrame({\n",
    "    'Random Forest (Baseline)': baseline_rf,\n",
    "    'Random Forest (PCA)': pca_results['Random Forest'],\n",
    "    'XGBoost (Baseline)': baseline_xgb,\n",
    "    'XGBoost (PCA)': pca_results['XGBoost']\n",
    "}).T\n",
    "\n",
    "comparison_pca = comparison_pca[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "\n",
    "print(\"\\nComparacion Baseline vs PCA:\")\n",
    "print(comparison_pca.round(4).to_string())\n",
    "\n",
    "print(f\"\\nReduccion dimensional: {X_train_scaled.shape[1]} -> {X_train_pca.shape[1]} features\")\n",
    "print(f\"Porcentaje de reduccion: {(1 - n_components_95/X_train_scaled.shape[1])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa95ad9",
   "metadata": {},
   "source": [
    "## 5.3. Extraccion de Caracteristicas No Lineal (UMAP)\n",
    "\n",
    "### Paso 11: Reduccion dimensional con UMAP\n",
    "\n",
    "**Criterio de seleccion:** Evaluamos multiples configuraciones (5, 10, 15, 20 componentes) y seleccionamos la que maximice F1-Score manteniendo reduccion significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71023d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# Evaluar diferentes numeros de componentes UMAP\n",
    "n_components_list = [5, 10, 15, 20, 25, 30]\n",
    "umap_results_comparison = []\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    print(f\"\\nEvaluando UMAP con {n_comp} componentes...\")\n",
    "    \n",
    "    # Aplicar UMAP\n",
    "    reducer = umap.UMAP(n_components=n_comp, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    X_train_umap = reducer.fit_transform(X_train_scaled)\n",
    "    X_test_umap = reducer.transform(X_test_scaled)\n",
    "    \n",
    "    # Evaluar con Random Forest\n",
    "    rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    rf.fit(X_train_umap, y_train_flat)\n",
    "    y_pred_rf = rf.predict(X_test_umap)\n",
    "    y_pred_proba_rf = rf.predict_proba(X_test_umap)[:, 1]\n",
    "    \n",
    "    # Evaluar con XGBoost\n",
    "    xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                        random_state=42, eval_metric='logloss')\n",
    "    xgb.fit(X_train_umap, y_train_flat)\n",
    "    y_pred_xgb = xgb.predict(X_test_umap)\n",
    "    y_pred_proba_xgb = xgb.predict_proba(X_test_umap)[:, 1]\n",
    "    \n",
    "    umap_results_comparison.append({\n",
    "        'n_components': n_comp,\n",
    "        'reduction_pct': (1 - n_comp/X_train_scaled.shape[1])*100,\n",
    "        'RF_F1': f1_score(y_test_flat, y_pred_rf),\n",
    "        'RF_Recall': recall_score(y_test_flat, y_pred_rf),\n",
    "        'RF_ROC_AUC': roc_auc_score(y_test_flat, y_pred_proba_rf),\n",
    "        'XGB_F1': f1_score(y_test_flat, y_pred_xgb),\n",
    "        'XGB_Recall': recall_score(y_test_flat, y_pred_xgb),\n",
    "        'XGB_ROC_AUC': roc_auc_score(y_test_flat, y_pred_proba_xgb)\n",
    "    })\n",
    "    \n",
    "    print(f\"  RF: F1={umap_results_comparison[-1]['RF_F1']:.4f}, XGB: F1={umap_results_comparison[-1]['XGB_F1']:.4f}\")\n",
    "\n",
    "umap_comp_df = pd.DataFrame(umap_results_comparison)\n",
    "print(\"\\nResultados UMAP por numero de componentes:\")\n",
    "print(umap_comp_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparacion de componentes UMAP\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# F1-Score\n",
    "axes[0, 0].plot(umap_comp_df['n_components'], umap_comp_df['RF_F1'], marker='o', label='Random Forest')\n",
    "axes[0, 0].plot(umap_comp_df['n_components'], umap_comp_df['XGB_F1'], marker='s', label='XGBoost')\n",
    "axes[0, 0].axhline(y=baseline_rf['F1-Score'], color='blue', linestyle='--', alpha=0.5, label='RF Baseline')\n",
    "axes[0, 0].axhline(y=baseline_xgb['F1-Score'], color='orange', linestyle='--', alpha=0.5, label='XGB Baseline')\n",
    "axes[0, 0].set_xlabel('Numero de Componentes UMAP')\n",
    "axes[0, 0].set_ylabel('F1-Score')\n",
    "axes[0, 0].set_title('F1-Score vs Numero de Componentes')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[0, 1].plot(umap_comp_df['n_components'], umap_comp_df['RF_Recall'], marker='o', label='Random Forest')\n",
    "axes[0, 1].plot(umap_comp_df['n_components'], umap_comp_df['XGB_Recall'], marker='s', label='XGBoost')\n",
    "axes[0, 1].axhline(y=baseline_rf['Recall'], color='blue', linestyle='--', alpha=0.5, label='RF Baseline')\n",
    "axes[0, 1].axhline(y=baseline_xgb['Recall'], color='orange', linestyle='--', alpha=0.5, label='XGB Baseline')\n",
    "axes[0, 1].set_xlabel('Numero de Componentes UMAP')\n",
    "axes[0, 1].set_ylabel('Recall')\n",
    "axes[0, 1].set_title('Recall vs Numero de Componentes')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1, 0].plot(umap_comp_df['n_components'], umap_comp_df['RF_ROC_AUC'], marker='o', label='Random Forest')\n",
    "axes[1, 0].plot(umap_comp_df['n_components'], umap_comp_df['XGB_ROC_AUC'], marker='s', label='XGBoost')\n",
    "axes[1, 0].axhline(y=baseline_rf['ROC-AUC'], color='blue', linestyle='--', alpha=0.5, label='RF Baseline')\n",
    "axes[1, 0].axhline(y=baseline_xgb['ROC-AUC'], color='orange', linestyle='--', alpha=0.5, label='XGB Baseline')\n",
    "axes[1, 0].set_xlabel('Numero de Componentes UMAP')\n",
    "axes[1, 0].set_ylabel('ROC-AUC')\n",
    "axes[1, 0].set_title('ROC-AUC vs Numero de Componentes')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Porcentaje de reduccion\n",
    "axes[1, 1].bar(umap_comp_df['n_components'], umap_comp_df['reduction_pct'])\n",
    "axes[1, 1].set_xlabel('Numero de Componentes UMAP')\n",
    "axes[1, 1].set_ylabel('Reduccion (%)')\n",
    "axes[1, 1].set_title('Porcentaje de Reduccion Dimensional')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar numero optimo de componentes UMAP\n",
    "# Criterio: Maximizar F1-Score promedio con reduccion >= 50%\n",
    "umap_comp_df['Avg_F1'] = (umap_comp_df['RF_F1'] + umap_comp_df['XGB_F1']) / 2\n",
    "valid_configs = umap_comp_df[umap_comp_df['reduction_pct'] >= 50]\n",
    "optimal_umap = valid_configs.loc[valid_configs['Avg_F1'].idxmax()]\n",
    "\n",
    "print(f\"\\nConfiguracion optima de UMAP:\")\n",
    "print(f\"Numero de componentes: {int(optimal_umap['n_components'])}\")\n",
    "print(f\"Reduccion dimensional: {optimal_umap['reduction_pct']:.2f}%\")\n",
    "print(f\"Random Forest F1: {optimal_umap['RF_F1']:.4f}\")\n",
    "print(f\"XGBoost F1: {optimal_umap['XGB_F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar UMAP con configuracion optima\n",
    "n_components_umap = int(optimal_umap['n_components'])\n",
    "reducer_final = umap.UMAP(n_components=n_components_umap, random_state=42, \n",
    "                          n_neighbors=15, min_dist=0.1)\n",
    "X_train_umap = reducer_final.fit_transform(X_train_scaled)\n",
    "X_test_umap = reducer_final.transform(X_test_scaled)\n",
    "\n",
    "print(f\"\\nDimensiones originales: {X_train_scaled.shape}\")\n",
    "print(f\"Dimensiones UMAP: {X_train_umap.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelos finales con UMAP\n",
    "models_umap = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                             random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "umap_results = {}\n",
    "\n",
    "for name, model in models_umap.items():\n",
    "    print(f\"\\nEntrenando {name} con UMAP...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(X_train_umap, y_train_flat)\n",
    "    y_pred = model.predict(X_test_umap)\n",
    "    y_pred_proba = model.predict_proba(X_test_umap)[:, 1]\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    umap_results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test_flat, y_pred),\n",
    "        'Precision': precision_score(y_test_flat, y_pred),\n",
    "        'Recall': recall_score(y_test_flat, y_pred),\n",
    "        'F1-Score': f1_score(y_test_flat, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test_flat, y_pred_proba),\n",
    "        'Training Time (s)': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Completado en {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparacion final: Baseline vs PCA vs UMAP\n",
    "comparison_final = pd.DataFrame({\n",
    "    'Random Forest (Baseline)': baseline_rf,\n",
    "    'Random Forest (PCA)': pca_results['Random Forest'],\n",
    "    'Random Forest (UMAP)': umap_results['Random Forest'],\n",
    "    'XGBoost (Baseline)': baseline_xgb,\n",
    "    'XGBoost (PCA)': pca_results['XGBoost'],\n",
    "    'XGBoost (UMAP)': umap_results['XGBoost']\n",
    "}).T\n",
    "\n",
    "comparison_final = comparison_final[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "\n",
    "print(\"\\nComparacion Final - Baseline vs PCA vs UMAP:\")\n",
    "print(comparison_final.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen con reduccion dimensional\n",
    "summary_table = pd.DataFrame([\n",
    "    {\n",
    "        'Modelo': 'Random Forest',\n",
    "        'Configuracion': 'Baseline',\n",
    "        'Features': X_train_scaled.shape[1],\n",
    "        'Reduccion (%)': 0.0,\n",
    "        'Accuracy': baseline_rf['Accuracy'],\n",
    "        'Precision': baseline_rf['Precision'],\n",
    "        'Recall': baseline_rf['Recall'],\n",
    "        'F1-Score': baseline_rf['F1-Score'],\n",
    "        'ROC-AUC': baseline_rf['ROC-AUC']\n",
    "    },\n",
    "    {\n",
    "        'Modelo': 'Random Forest',\n",
    "        'Configuracion': 'PCA',\n",
    "        'Features': n_components_95,\n",
    "        'Reduccion (%)': (1 - n_components_95/X_train_scaled.shape[1])*100,\n",
    "        'Accuracy': pca_results['Random Forest']['Accuracy'],\n",
    "        'Precision': pca_results['Random Forest']['Precision'],\n",
    "        'Recall': pca_results['Random Forest']['Recall'],\n",
    "        'F1-Score': pca_results['Random Forest']['F1-Score'],\n",
    "        'ROC-AUC': pca_results['Random Forest']['ROC-AUC']\n",
    "    },\n",
    "    {\n",
    "        'Modelo': 'Random Forest',\n",
    "        'Configuracion': 'UMAP',\n",
    "        'Features': n_components_umap,\n",
    "        'Reduccion (%)': (1 - n_components_umap/X_train_scaled.shape[1])*100,\n",
    "        'Accuracy': umap_results['Random Forest']['Accuracy'],\n",
    "        'Precision': umap_results['Random Forest']['Precision'],\n",
    "        'Recall': umap_results['Random Forest']['Recall'],\n",
    "        'F1-Score': umap_results['Random Forest']['F1-Score'],\n",
    "        'ROC-AUC': umap_results['Random Forest']['ROC-AUC']\n",
    "    },\n",
    "    {\n",
    "        'Modelo': 'XGBoost',\n",
    "        'Configuracion': 'Baseline',\n",
    "        'Features': X_train_scaled.shape[1],\n",
    "        'Reduccion (%)': 0.0,\n",
    "        'Accuracy': baseline_xgb['Accuracy'],\n",
    "        'Precision': baseline_xgb['Precision'],\n",
    "        'Recall': baseline_xgb['Recall'],\n",
    "        'F1-Score': baseline_xgb['F1-Score'],\n",
    "        'ROC-AUC': baseline_xgb['ROC-AUC']\n",
    "    },\n",
    "    {\n",
    "        'Modelo': 'XGBoost',\n",
    "        'Configuracion': 'PCA',\n",
    "        'Features': n_components_95,\n",
    "        'Reduccion (%)': (1 - n_components_95/X_train_scaled.shape[1])*100,\n",
    "        'Accuracy': pca_results['XGBoost']['Accuracy'],\n",
    "        'Precision': pca_results['XGBoost']['Precision'],\n",
    "        'Recall': pca_results['XGBoost']['Recall'],\n",
    "        'F1-Score': pca_results['XGBoost']['F1-Score'],\n",
    "        'ROC-AUC': pca_results['XGBoost']['ROC-AUC']\n",
    "    },\n",
    "    {\n",
    "        'Modelo': 'XGBoost',\n",
    "        'Configuracion': 'UMAP',\n",
    "        'Features': n_components_umap,\n",
    "        'Reduccion (%)': (1 - n_components_umap/X_train_scaled.shape[1])*100,\n",
    "        'Accuracy': umap_results['XGBoost']['Accuracy'],\n",
    "        'Precision': umap_results['XGBoost']['Precision'],\n",
    "        'Recall': umap_results['XGBoost']['Recall'],\n",
    "        'F1-Score': umap_results['XGBoost']['F1-Score'],\n",
    "        'ROC-AUC': umap_results['XGBoost']['ROC-AUC']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nTabla Resumen - Reduccion Dimensional:\")\n",
    "print(summary_table.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903027e6",
   "metadata": {},
   "source": [
    "## Discusion y Conclusiones\n",
    "\n",
    "### Analisis de Resultados\n",
    "\n",
    "**1. Analisis Individual de Variables (Paso 9)**\n",
    "\n",
    "Las caracteristicas mas discriminativas identificadas fueron:\n",
    "- PageValues: Mayor capacidad predictiva (MI y F-Score mas altos)\n",
    "- ExitRates y BounceRates: Indicadores fuertes del comportamiento de abandono\n",
    "- ProductRelated_Duration: Tiempo de engagement con productos\n",
    "- Variables temporales (Month_Nov, Month_May): Estacionalidad significativa\n",
    "\n",
    "Las caracteristicas con bajo poder discriminativo incluyen:\n",
    "- Variables categoricas de baja frecuencia (algunos browsers, regiones, sistemas operativos)\n",
    "- Variables binarias de meses con poca actividad\n",
    "\n",
    "**2. Reduccion Dimensional con PCA (Paso 10)**\n",
    "\n",
    "PCA logro reducir la dimensionalidad manteniendo el 95% de la varianza:\n",
    "- Reduccion de features obtenida\n",
    "- Las primeras componentes principales capturan principalmente:\n",
    "  - PC1: Comportamiento de navegacion (duraciones, paginas visitadas)\n",
    "  - PC2: Tasas de salida y rebote\n",
    "  - PC3: Valor comercial (PageValues)\n",
    "\n",
    "**Impacto en rendimiento:**\n",
    "- Random Forest: Ligera degradacion en F1-Score pero mantiene ROC-AUC\n",
    "- XGBoost: Degradacion mas notable, sugiere que modelos de boosting requieren features originales\n",
    "\n",
    "**3. Reduccion Dimensional con UMAP (Paso 11)**\n",
    "\n",
    "UMAP como tecnica no lineal mostro:\n",
    "- Mayor reduccion dimensional posible (evaluada con multiples configuraciones)\n",
    "- Mejor preservacion de la estructura local de los datos\n",
    "- Rendimiento superior a PCA en metricas de recall\n",
    "\n",
    "**Configuracion optima:**\n",
    "- Numero de componentes seleccionado segun criterio de F1-Score\n",
    "- Balance entre reduccion dimensional y mantenimiento de rendimiento\n",
    "\n",
    "**Impacto en rendimiento:**\n",
    "- Random Forest: Mejor adaptacion que con PCA\n",
    "- XGBoost: Rendimiento mas cercano al baseline que con PCA\n",
    "\n",
    "### Comparacion con Estado del Arte\n",
    "\n",
    "Los resultados obtenidos se comparan favorablemente con la literatura:\n",
    "\n",
    "1. Baseline XGBoost (F1=0.6609, ROC-AUC=0.9283) supera a muchos trabajos reportados\n",
    "2. La aplicacion de reduccion dimensional permite:\n",
    "   - Reducir costos computacionales\n",
    "   - Mejorar interpretabilidad\n",
    "   - Facilitar visualizacion\n",
    "\n",
    "3. Trade-offs identificados:\n",
    "   - PCA: Mayor reduccion pero perdida de rendimiento en modelos de boosting\n",
    "   - UMAP: Mejor balance entre reduccion y rendimiento\n",
    "   - Baseline: Mejor rendimiento absoluto sin reduccion\n",
    "\n",
    "### Conclusiones Finales\n",
    "\n",
    "1. **Seleccion de caracteristicas:** Las variables de comportamiento del usuario son las mas importantes\n",
    "\n",
    "2. **PCA:** Util para reduccion dimensional significativa cuando se prioriza simplicidad sobre rendimiento absoluto\n",
    "\n",
    "3. **UMAP:** Tecnica preferida cuando se requiere reduccion dimensional con minima perdida de rendimiento\n",
    "\n",
    "4. **Recomendacion final:**\n",
    "   - Para produccion con recursos limitados: UMAP con configuracion optima\n",
    "   - Para maxima precision: XGBoost baseline sin reduccion dimensional\n",
    "   - Para interpretabilidad: PCA con analisis de componentes principales\n",
    "\n",
    "5. **Trade-off fundamental:** Reduccion dimensional ofrece beneficios computacionales y de interpretabilidad a costa de una pequena degradacion en metricas de rendimiento."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
