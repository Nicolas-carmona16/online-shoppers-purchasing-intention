{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "114461cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ucimlrepo) (2024.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nicolas\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ucimlrepo matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "358c58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70859f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "online_shoppers_purchasing_intention_dataset = fetch_ucirepo(id=468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d44e6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (as pandas dataframes)\n",
    "X = online_shoppers_purchasing_intention_dataset.data.features\n",
    "y = online_shoppers_purchasing_intention_dataset.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ef7db54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 468, 'name': 'Online Shoppers Purchasing Intention Dataset', 'repository_url': 'https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset', 'data_url': 'https://archive.ics.uci.edu/static/public/468/data.csv', 'abstract': 'Of the 12,330 sessions in the dataset,\\n84.5% (10,422) were negative class samples that did not\\nend with shopping, and the rest (1908) were positive class\\nsamples ending with shopping.', 'area': 'Business', 'tasks': ['Classification', 'Clustering'], 'characteristics': ['Multivariate'], 'num_instances': 12330, 'num_features': 17, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Revenue'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2018, 'last_updated': 'Thu Jan 11 2024', 'dataset_doi': '10.24432/C5F88Q', 'creators': ['C. Sakar', 'Yomi Kastro'], 'intro_paper': {'ID': 367, 'type': 'NATIVE', 'title': 'Real-time prediction of online shoppers‚Äô purchasing intention using multilayer perceptron and LSTM recurrent neural networks', 'authors': 'C. O. Sakar, S. Polat, Mete Katircioglu, Yomi Kastro', 'venue': 'Neural computing & applications (Print)', 'year': 2019, 'journal': None, 'DOI': '10.1007/s00521-018-3523-0', 'URL': 'https://www.semanticscholar.org/paper/747e098f85ca2d20afd6313b11242c0c427e6fb3', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The dataset consists of feature vectors belonging to 12,330 sessions. \\r\\nThe dataset was formed so that each session\\r\\nwould belong to a different user in a 1-year period to avoid\\r\\nany tendency to a specific campaign, special day, user\\r\\nprofile, or period. ', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The dataset consists of 10 numerical and 8 categorical attributes.\\r\\nThe \\'Revenue\\' attribute can be used as the class label.\\r\\n\\r\\n\"Administrative\", \"Administrative Duration\", \"Informational\", \"Informational Duration\", \"Product Related\" and \"Product Related Duration\" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. The \"Bounce Rate\", \"Exit Rate\" and \"Page Value\" features represent the metrics measured by \"Google Analytics\" for each page in the e-commerce site. The value of \"Bounce Rate\" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session. The value of \"Exit Rate\" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. The \"Page Value\" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. The \"Special Day\" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother‚Äôs Day, Valentine\\'s Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina‚Äôs day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. The dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.', 'citation': None}}\n"
     ]
    }
   ],
   "source": [
    "# metadata\n",
    "print(online_shoppers_purchasing_intention_dataset.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "360e5ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name     role         type demographic description  \\\n",
      "0            Administrative  Feature      Integer        None        None   \n",
      "1   Administrative_Duration  Feature      Integer        None        None   \n",
      "2             Informational  Feature      Integer        None        None   \n",
      "3    Informational_Duration  Feature      Integer        None        None   \n",
      "4            ProductRelated  Feature      Integer        None        None   \n",
      "5   ProductRelated_Duration  Feature   Continuous        None        None   \n",
      "6               BounceRates  Feature   Continuous        None        None   \n",
      "7                 ExitRates  Feature   Continuous        None        None   \n",
      "8                PageValues  Feature      Integer        None        None   \n",
      "9                SpecialDay  Feature      Integer        None        None   \n",
      "10                    Month  Feature  Categorical        None        None   \n",
      "11         OperatingSystems  Feature      Integer        None        None   \n",
      "12                  Browser  Feature      Integer        None        None   \n",
      "13                   Region  Feature      Integer        None        None   \n",
      "14              TrafficType  Feature      Integer        None        None   \n",
      "15              VisitorType  Feature  Categorical        None        None   \n",
      "16                  Weekend  Feature       Binary        None        None   \n",
      "17                  Revenue   Target       Binary        None        None   \n",
      "\n",
      "   units missing_values  \n",
      "0   None             no  \n",
      "1   None             no  \n",
      "2   None             no  \n",
      "3   None             no  \n",
      "4   None             no  \n",
      "5   None             no  \n",
      "6   None             no  \n",
      "7   None             no  \n",
      "8   None             no  \n",
      "9   None             no  \n",
      "10  None             no  \n",
      "11  None             no  \n",
      "12  None             no  \n",
      "13  None             no  \n",
      "14  None             no  \n",
      "15  None             no  \n",
      "16  None             no  \n",
      "17  None             no  \n"
     ]
    }
   ],
   "source": [
    "# variable information\n",
    "print(online_shoppers_purchasing_intention_dataset.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0c10e",
   "metadata": {},
   "source": [
    "# FASE 1: PREPARACI√ìN DE DATOS\n",
    "\n",
    "## Paso 1.1: Divisi√≥n Train/Test (Estratificada)\n",
    "\n",
    "Usamos una configuraci√≥n de 80% entrenamiento y 20% test, de esta forma queda de la siguiente manera: \n",
    "- 9,864 train / 2,466 test\n",
    "- Desbalance 85/15: ~370 casos positivos en test (suficiente para evaluaci√≥n confiable)\n",
    "- Suficientes datos para entrenar y aplicar SMOTE posteriormente\n",
    "\n",
    "Adem√°s, con la estratificaci√≥n se mantiene la proporci√≥n 85/15 en ambos conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bd0bc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o de los conjuntos:\n",
      "Train: 9,864 muestras (80.0%)\n",
      "Test:  2,466 muestras (20.0%)\n",
      "\n",
      "Distribuci√≥n de clases en TRAIN:\n",
      "False (No compra): 8,338 (84.53%)\n",
      "True (Compra):     1,526 (15.47%)\n",
      "\n",
      "Distribuci√≥n de clases en TEST:\n",
      "False (No compra): 2,084 (84.51%)\n",
      "True (Compra):     382 (15.49%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.20,           # Definimos el 20% para test\n",
    "    random_state=42,          # Reproducibilidad\n",
    "    stratify=y                # Mantiene proporci√≥n de clases\n",
    ")\n",
    "\n",
    "print(f\"Tama√±o de los conjuntos:\")\n",
    "print(f\"Train: {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test:  {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de clases en TRAIN:\")\n",
    "train_counts = y_train['Revenue'].value_counts()\n",
    "print(f\"False (No compra): {train_counts[False]:,} ({train_counts[False]/len(y_train)*100:.2f}%)\")\n",
    "print(f\"True (Compra):     {train_counts[True]:,} ({train_counts[True]/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de clases en TEST:\")\n",
    "test_counts = y_test['Revenue'].value_counts()\n",
    "print(f\"False (No compra): {test_counts[False]:,} ({test_counts[False]/len(y_test)*100:.2f}%)\")\n",
    "print(f\"True (Compra):     {test_counts[True]:,} ({test_counts[True]/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2faa12",
   "metadata": {},
   "source": [
    "## Paso 1.2: Codificaci√≥n de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc2a52ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificaci√≥n completada\n",
      "X_train: (9864, 43)\n",
      "X_test:  (2466, 43)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "# 1. WEEKEND: Bool ‚Üí Int\n",
    "X_train_encoded['Weekend'] = X_train_encoded['Weekend'].astype(int)\n",
    "X_test_encoded['Weekend'] = X_test_encoded['Weekend'].astype(int)\n",
    "\n",
    "# 2. MONTH: OneHot\n",
    "month_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "month_encoded_train = month_encoder.fit_transform(X_train_encoded[['Month']])\n",
    "month_encoded_test = month_encoder.transform(X_test_encoded[['Month']])\n",
    "month_cols = [f'Month_{cat}' for cat in month_encoder.categories_[0][1:]]\n",
    "month_train_df = pd.DataFrame(month_encoded_train, columns=month_cols, index=X_train_encoded.index)\n",
    "month_test_df = pd.DataFrame(month_encoded_test, columns=month_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('Month', axis=1), month_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('Month', axis=1), month_test_df], axis=1)\n",
    "\n",
    "# 3. VISITORTYPE: OneHot\n",
    "visitor_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "visitor_encoded_train = visitor_encoder.fit_transform(X_train_encoded[['VisitorType']])\n",
    "visitor_encoded_test = visitor_encoder.transform(X_test_encoded[['VisitorType']])\n",
    "visitor_cols = [f'VisitorType_{cat}' for cat in visitor_encoder.categories_[0][1:]]\n",
    "visitor_train_df = pd.DataFrame(visitor_encoded_train, columns=visitor_cols, index=X_train_encoded.index)\n",
    "visitor_test_df = pd.DataFrame(visitor_encoded_test, columns=visitor_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('VisitorType', axis=1), visitor_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('VisitorType', axis=1), visitor_test_df], axis=1)\n",
    "\n",
    "# 4. OPERATINGSYSTEMS: OneHot\n",
    "os_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "os_encoded_train = os_encoder.fit_transform(X_train_encoded[['OperatingSystems']])\n",
    "os_encoded_test = os_encoder.transform(X_test_encoded[['OperatingSystems']])\n",
    "os_cols = [f'OS_{int(cat)}' for cat in os_encoder.categories_[0][1:]]\n",
    "os_train_df = pd.DataFrame(os_encoded_train, columns=os_cols, index=X_train_encoded.index)\n",
    "os_test_df = pd.DataFrame(os_encoded_test, columns=os_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('OperatingSystems', axis=1), os_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('OperatingSystems', axis=1), os_test_df], axis=1)\n",
    "\n",
    "# 5. BROWSER: OneHot con Grouping (Top 5 + Other)\n",
    "top_5_browsers = X_train_encoded['Browser'].value_counts().head(5).index.tolist()\n",
    "X_train_encoded['Browser_grouped'] = X_train_encoded['Browser'].apply(\n",
    "    lambda x: x if x in top_5_browsers else 99\n",
    ")\n",
    "X_test_encoded['Browser_grouped'] = X_test_encoded['Browser'].apply(\n",
    "    lambda x: x if x in top_5_browsers else 99\n",
    ")\n",
    "browser_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "browser_encoded_train = browser_encoder.fit_transform(X_train_encoded[['Browser_grouped']])\n",
    "browser_encoded_test = browser_encoder.transform(X_test_encoded[['Browser_grouped']])\n",
    "browser_cols = [f'Browser_{int(cat) if cat != 99 else \"Other\"}' for cat in browser_encoder.categories_[0][1:]]\n",
    "browser_train_df = pd.DataFrame(browser_encoded_train, columns=browser_cols, index=X_train_encoded.index)\n",
    "browser_test_df = pd.DataFrame(browser_encoded_test, columns=browser_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop(['Browser', 'Browser_grouped'], axis=1), browser_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop(['Browser', 'Browser_grouped'], axis=1), browser_test_df], axis=1)\n",
    "\n",
    "# 6. REGION: OneHot\n",
    "region_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "region_encoded_train = region_encoder.fit_transform(X_train_encoded[['Region']])\n",
    "region_encoded_test = region_encoder.transform(X_test_encoded[['Region']])\n",
    "region_cols = [f'Region_{int(cat)}' for cat in region_encoder.categories_[0][1:]]\n",
    "region_train_df = pd.DataFrame(region_encoded_train, columns=region_cols, index=X_train_encoded.index)\n",
    "region_test_df = pd.DataFrame(region_encoded_test, columns=region_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('Region', axis=1), region_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('Region', axis=1), region_test_df], axis=1)\n",
    "\n",
    "# 7. TRAFFICTYPE: Target Encoding\n",
    "traffic_conversion_rate = X_train_encoded.join(y_train).groupby('TrafficType')['Revenue'].mean().to_dict()\n",
    "global_mean = y_train['Revenue'].mean()\n",
    "X_train_encoded['TrafficType_Encoded'] = X_train_encoded['TrafficType'].map(traffic_conversion_rate)\n",
    "X_test_encoded['TrafficType_Encoded'] = X_test_encoded['TrafficType'].map(traffic_conversion_rate).fillna(global_mean)\n",
    "X_train_encoded = X_train_encoded.drop('TrafficType', axis=1)\n",
    "X_test_encoded = X_test_encoded.drop('TrafficType', axis=1)\n",
    "\n",
    "print(f\"Codificaci√≥n completada\")\n",
    "print(f\"X_train: {X_train_encoded.shape}\")\n",
    "print(f\"X_test:  {X_test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46df8c",
   "metadata": {},
   "source": [
    "### Resumen de Codificaci√≥n Completada\n",
    "\n",
    "**Transformaciones aplicadas:**\n",
    "\n",
    "| Variable Original | Estrategia | Columnas Generadas | Justificaci√≥n |\n",
    "|------------------|------------|-------------------|---------------|\n",
    "| **Month** | OneHot (drop first) | 9 | Sin orden natural, captura estacionalidad |\n",
    "| **VisitorType** | OneHot (drop first) | 2 | Solo 3 categor√≠as nominales |\n",
    "| **Weekend** | Bool ‚Üí Int | 1 | Ya binaria, solo conversi√≥n |\n",
    "| **OperatingSystems** | OneHot (drop first) | 7 | 8 valores manejables |\n",
    "| **Browser** | OneHot + Grouping | 5 | Top 5 + \"Other\" (reducido de 13) |\n",
    "| **Region** | OneHot (drop first) | 8 | 9 valores geogr√°ficos |\n",
    "| **TrafficType** | Target Encoding | 1 | 20 valores ‚Üí 1 num√©rica |\n",
    "\n",
    "**Resultado:**\n",
    "- Features originales: 17\n",
    "- Features despu√©s de codificaci√≥n: **43** (vs 73 con OneHot completo)\n",
    "- Reducci√≥n de dimensionalidad: 41% menos features\n",
    "- Todos los encoders ajustados SOLO con train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1795d8",
   "metadata": {},
   "source": [
    "## Paso 1.3: Escalado de Variables Num√©ricas\n",
    "\n",
    "Usaremos **RobustScaler** porque en el an√°lisis exploratorio se vieron muchos outliers en variables de duraci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c78b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled: (9864, 43)\n",
      "X_test_scaled:  (2466, 43)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "numerical_cols_to_scale = [\n",
    "    'Administrative', 'Administrative_Duration',\n",
    "    'Informational', 'Informational_Duration',\n",
    "    'ProductRelated', 'ProductRelated_Duration',\n",
    "    'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay',\n",
    "    'TrafficType_Encoded'\n",
    "]\n",
    "\n",
    "X_train_scaled = X_train_encoded.copy()\n",
    "X_test_scaled = X_test_encoded.copy()\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_train_encoded[numerical_cols_to_scale])\n",
    "X_test_scaled[numerical_cols_to_scale] = scaler.transform(X_test_encoded[numerical_cols_to_scale])\n",
    "\n",
    "print(f\"X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled:  {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538985e6",
   "metadata": {},
   "source": [
    "### Resumen de la fase 1: Preparaci√≥n de Datos\n",
    "\n",
    "| Paso | Acci√≥n |\n",
    "|------|--------|\n",
    "| 1.1 | Divisi√≥n Train/Test (80/20 estratificado) |\n",
    "| 1.2 | Codificaci√≥n de variables categ√≥ricas |\n",
    "| 1.3 | Escalado de variables num√©ricas (RobustScaler) |\n",
    "\n",
    "**Datasets listos para entrenamiento:**\n",
    "- `X_train_scaled`: 9,864 muestras √ó 43 features\n",
    "- `X_test_scaled`: 2,466 muestras √ó 43 features\n",
    "- `y_train`: 9,864 etiquetas (84.5% No compra, 15.5% Compra)\n",
    "- `y_test`: 2,466 etiquetas (84.5% No compra, 15.5% Compra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb98ea0d",
   "metadata": {},
   "source": [
    "# FASE 2: MODELO BASELINE\n",
    "\n",
    "## Paso 2.1: Entrenamiento de Modelos sin Balanceo\n",
    "\n",
    "Entrenamos varios modelos con los datos desbalanceados (84.5% No compra / 15.5% Compra) para establecer una l√≠nea base, esto tambiem nos permitir√° comparar el efecto de SMOTE posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa9e6f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2c1afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Aplanar y_train y y_test para compatibilidad\n",
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "baseline_results = {}\n",
    "\n",
    "# Modelos a entrenar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=2000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train_scaled, y_train_flat)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # M√©tricas\n",
    "    accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "    precision = precision_score(y_test_flat, y_pred)\n",
    "    recall = recall_score(y_test_flat, y_pred)\n",
    "    f1 = f1_score(y_test_flat, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test_flat, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Resultados\n",
    "    baseline_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Model': model\n",
    "    }\n",
    "    \n",
    "    roc_auc_str = f\"{roc_auc:.4f}\" if roc_auc is not None else \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e34381",
   "metadata": {},
   "source": [
    "## Paso 2.2: Evaluaci√≥n y Comparaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d288b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy Precision    Recall  F1-Score   ROC-AUC Training Time (s)\n",
      "Logistic Regression  0.882401  0.755556  0.356021  0.483986  0.886398          1.598279\n",
      "Decision Tree        0.856853  0.537662  0.541885  0.539765  0.728236          0.110003\n",
      "Random Forest        0.896594  0.739623  0.513089  0.605873  0.918734          1.610435\n",
      "SVM                   0.88159  0.692308  0.424084  0.525974  0.851294         10.605659\n",
      "KNN                  0.886456  0.675862  0.513089  0.583333  0.848243          0.091994\n",
      "XGBoost              0.903082  0.721362  0.609948  0.660993  0.928317          0.168003\n",
      "\n",
      "Mejor modelo (por F1-Score): XGBoost\n",
      "F1-Score: 0.6610\n",
      "ROC-AUC: 0.9283\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(baseline_results).T\n",
    "results_df = results_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Training Time (s)']]\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Identificar mejor modelo por F1-Score (ya que es m√°s apropiado para datos desbalanceados)\n",
    "best_model_name = results_df['F1-Score'].idxmax()\n",
    "print(f\"\\nMejor modelo (por F1-Score): {best_model_name}\")\n",
    "print(f\"F1-Score: {results_df.loc[best_model_name, 'F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {results_df.loc[best_model_name, 'ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73ad744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An√°lisis de XGBoost\n",
      "\n",
      "Matriz de Confusi√≥n:\n",
      "                  Predicho: No Compra | Predicho: Compra\n",
      "Real: No Compra            1994     |         90\n",
      "Real: Compra                149     |        233\n",
      "\n",
      "Reporte de Clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Compra       0.93      0.96      0.94      2084\n",
      "      Compra       0.72      0.61      0.66       382\n",
      "\n",
      "    accuracy                           0.90      2466\n",
      "   macro avg       0.83      0.78      0.80      2466\n",
      "weighted avg       0.90      0.90      0.90      2466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matriz de confusi√≥n y reporte detallado del mejor modelo\n",
    "best_model = baseline_results[best_model_name]['Model']\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"An√°lisis de {best_model_name}\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test_flat, y_pred_best)\n",
    "print(\"\\nMatriz de Confusi√≥n:\")\n",
    "print(f\"                  Predicho: No Compra | Predicho: Compra\")\n",
    "print(f\"Real: No Compra          {cm[0][0]:6d}     |     {cm[0][1]:6d}\")\n",
    "print(f\"Real: Compra             {cm[1][0]:6d}     |     {cm[1][1]:6d}\")\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(\"\\nReporte de Clasificaci√≥n:\")\n",
    "print(classification_report(y_test_flat, y_pred_best, target_names=['No Compra', 'Compra']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f942d1",
   "metadata": {},
   "source": [
    "### Resumen FASE 2: Modelo Baseline\n",
    "\n",
    "**Resultados obtenidos Sin balanceo de clases:**\n",
    "\n",
    "| Modelo | Accuracy | Precision | Recall | F1-Score | ROC-AUC |\n",
    "|--------|----------|-----------|--------|----------|---------|\n",
    "| Random Forest | 0.8966 | 0.7396 | 0.5131 | 0.6059 | 0.9187 |\n",
    "| KNN | 0.8865 | 0.6759 | 0.5131 | 0.5833 | 0.8482 |\n",
    "| Logistic Regression | 0.8824 | 0.7556 | 0.3560 | 0.4840 | 0.8862 |\n",
    "| SVM | 0.8816 | 0.6923 | 0.4241 | 0.5260 | 0.8513 |\n",
    "| Decision Tree | 0.8569 | 0.5377 | 0.5419 | 0.5398 | 0.7282 |\n",
    "| **XGBoost** | **0.9030** | **0.7213** | **0.6099** | **0.6609** | **0.9283** |\n",
    "\n",
    "**An√°lisis:**\n",
    "\n",
    "1. XGBoost es el mejor modelo con F1=0.6609 y ROC-AUC=0.9283\n",
    "2. Problema del desbalanceo es evidente:\n",
    "   - Alta accuracy (90.3%) pero bajo recall (60.9%) para clase positiva\n",
    "   - El modelo predice bien \"No Compra\" (97% recall) pero falla en \"Compra\" (51% recall)\n",
    "   - 186 falsos negativos (casi la mitad de las compras no detectadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc2f55",
   "metadata": {},
   "source": [
    "# FASE 3: SMOTE PROGRESIVO\n",
    "\n",
    "## Paso 3.1: Aplicaci√≥n de SMOTE Incremental\n",
    "\n",
    "Se aplica SMOTE incrementalmente generando 5%, 10% y 15% adicional de muestras de la clase minoritaria. SMOTE se aplica SOLO en el conjunto de entrenamiento.\n",
    "\n",
    "**Estado actual:**\n",
    "- Clase minoritaria (Compra): 1,526 muestras (15.47%)\n",
    "- Clase mayoritaria (No Compra): 8,338 muestras (84.53%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ccd11fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalar imbalanced-learn si no est√° instalado\n",
    "%pip install imbalanced-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad9c1d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMOTE 5%:\n",
      "- Muestras minoritarias: 2019 (19.49%)\n",
      "- sampling_strategy: 0.2421\n",
      "\n",
      " SMOTE 10%:\n",
      "- Muestras minoritarias: 2512 (23.15%)\n",
      "- sampling_strategy: 0.3013\n",
      "\n",
      "SMOTE 15%:\n",
      "- Muestras minoritarias: 3005 (26.49%)\n",
      "- sampling_strategy: 0.3604\n",
      "\n",
      "SMOTE 5%:\n",
      "Total muestras: 10357\n",
      "Compra (1): 2019 (19.49%)\n",
      "No Compra (0): 8338 (80.51%)\n",
      "\n",
      "SMOTE 10%:\n",
      "Total muestras: 10850\n",
      "Compra (1): 2512 (23.15%)\n",
      "No Compra (0): 8338 (76.85%)\n",
      "\n",
      "SMOTE 15%:\n",
      "Total muestras: 11343\n",
      "Compra (1): 3005 (26.49%)\n",
      "No Compra (0): 8338 (73.51%)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Estado actual de las clases en train\n",
    "current_minority = (y_train_flat == 1).sum()  # 1526\n",
    "current_majority = (y_train_flat == 0).sum()  # 8338\n",
    "total_train = len(y_train_flat)\n",
    "\n",
    "# Hacemos el Calculo sampling_strategy para cada nivel de SMOTE\n",
    "# sampling_strategy = num_samples_minority / num_samples_majority\n",
    "\n",
    "# SMOTE 5%\n",
    "smote_5_samples = current_minority + int(total_train * 0.05)\n",
    "strategy_5 = smote_5_samples / current_majority\n",
    "\n",
    "# SMOTE 10%\n",
    "smote_10_samples = current_minority + int(total_train * 0.10)\n",
    "strategy_10 = smote_10_samples / current_majority\n",
    "\n",
    "# SMOTE 15%\n",
    "smote_15_samples = current_minority + int(total_train * 0.15)\n",
    "strategy_15 = smote_15_samples / current_majority\n",
    "\n",
    "print(f\"\\nSMOTE 5%:\")\n",
    "print(f\"- Muestras minoritarias: {smote_5_samples} ({smote_5_samples/(current_majority+smote_5_samples)*100:.2f}%)\")\n",
    "print(f\"- sampling_strategy: {strategy_5:.4f}\")\n",
    "\n",
    "print(f\"\\n SMOTE 10%:\")\n",
    "print(f\"- Muestras minoritarias: {smote_10_samples} ({smote_10_samples/(current_majority+smote_10_samples)*100:.2f}%)\")\n",
    "print(f\"- sampling_strategy: {strategy_10:.4f}\")\n",
    "\n",
    "print(f\"\\nSMOTE 15%:\")\n",
    "print(f\"- Muestras minoritarias: {smote_15_samples} ({smote_15_samples/(current_majority+smote_15_samples)*100:.2f}%)\")\n",
    "print(f\"- sampling_strategy: {strategy_15:.4f}\")\n",
    "\n",
    "# Se aplica SMOTE para cada nivel\n",
    "smote_configs = {\n",
    "    'SMOTE 5%': (strategy_5, smote_5_samples),\n",
    "    'SMOTE 10%': (strategy_10, smote_10_samples),\n",
    "    'SMOTE 15%': (strategy_15, smote_15_samples)\n",
    "}\n",
    "\n",
    "smote_datasets = {}\n",
    "\n",
    "for name, (strategy, expected_samples) in smote_configs.items():\n",
    "    smote = SMOTE(sampling_strategy=strategy, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train_flat)\n",
    "    \n",
    "    smote_datasets[name] = {\n",
    "        'X': X_resampled,\n",
    "        'y': y_resampled,\n",
    "        'minority_count': (y_resampled == 1).sum(),\n",
    "        'majority_count': (y_resampled == 0).sum(),\n",
    "        'total': len(y_resampled)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Total muestras: {smote_datasets[name]['total']}\")\n",
    "    print(f\"Compra (1): {smote_datasets[name]['minority_count']} ({smote_datasets[name]['minority_count']/smote_datasets[name]['total']*100:.2f}%)\")\n",
    "    print(f\"No Compra (0): {smote_datasets[name]['majority_count']} ({smote_datasets[name]['majority_count']/smote_datasets[name]['total']*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bea9dd",
   "metadata": {},
   "source": [
    "## Paso 3.2: Entrenamiento con SMOTE y Evaluaci√≥n\n",
    "\n",
    "Entrenaremos los 6 modelos con cada configuraci√≥n de SMOTE y compararemos con el baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1028f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 5%\n",
      "  Logistic Regression  | F1: 0.5240 | Recall: 0.4136 | ROC-AUC: 0.8876\n",
      "  Decision Tree        | F1: 0.5181 | Recall: 0.5236 | ROC-AUC: 0.7162\n",
      "  Random Forest        | F1: 0.6372 | Recall: 0.5654 | ROC-AUC: 0.9166\n",
      "  SVM                  | F1: 0.5644 | Recall: 0.4817 | ROC-AUC: 0.8688\n",
      "  KNN                  | F1: 0.5863 | Recall: 0.5733 | ROC-AUC: 0.8437\n",
      "  XGBoost              | F1: 0.6535 | Recall: 0.6073 | ROC-AUC: 0.9302\n",
      "SMOTE 10%\n",
      "  Logistic Regression  | F1: 0.5639 | Recall: 0.4738 | ROC-AUC: 0.8878\n",
      "  Decision Tree        | F1: 0.5160 | Recall: 0.5288 | ROC-AUC: 0.7167\n",
      "  Random Forest        | F1: 0.6401 | Recall: 0.5681 | ROC-AUC: 0.9174\n",
      "  SVM                  | F1: 0.5865 | Recall: 0.5236 | ROC-AUC: 0.8672\n",
      "  KNN                  | F1: 0.5714 | Recall: 0.6178 | ROC-AUC: 0.8437\n",
      "  XGBoost              | F1: 0.6509 | Recall: 0.6126 | ROC-AUC: 0.9301\n",
      "SMOTE 15%\n",
      "  Logistic Regression  | F1: 0.5979 | Recall: 0.5236 | ROC-AUC: 0.8869\n",
      "  Decision Tree        | F1: 0.5768 | Recall: 0.6047 | ROC-AUC: 0.7573\n",
      "  Random Forest        | F1: 0.6304 | Recall: 0.5759 | ROC-AUC: 0.9160\n",
      "  SVM                  | F1: 0.5953 | Recall: 0.5602 | ROC-AUC: 0.8488\n",
      "  KNN                  | F1: 0.5889 | Recall: 0.6806 | ROC-AUC: 0.8511\n",
      "  XGBoost              | F1: 0.6475 | Recall: 0.6178 | ROC-AUC: 0.9279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import time\n",
    "\n",
    "smote_results = {}\n",
    "\n",
    "models_smote = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=2000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Entrenamos con cada configuraci√≥n de SMOTE\n",
    "for smote_name, smote_data in smote_datasets.items():\n",
    "    print(f\"{smote_name}\")\n",
    "    \n",
    "    X_smote = smote_data['X']\n",
    "    y_smote = smote_data['y']\n",
    "    \n",
    "    smote_results[smote_name] = {}\n",
    "    \n",
    "    for model_name, model in models_smote.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entrenar\n",
    "        model.fit(X_smote, y_smote)\n",
    "        \n",
    "        # Predecir en test (Recordar que el test es sin SMOTE)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # M√©tricas\n",
    "        accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "        precision = precision_score(y_test_flat, y_pred)\n",
    "        recall = recall_score(y_test_flat, y_pred)\n",
    "        f1 = f1_score(y_test_flat, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test_flat, y_pred_proba) if y_pred_proba is not None else None\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        smote_results[smote_name][model_name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'Training Time (s)': training_time\n",
    "        }\n",
    "        \n",
    "        roc_auc_str = f\"{roc_auc:.4f}\" if roc_auc is not None else \"N/A\"\n",
    "        print(f\"  {model_name:20s} | F1: {f1:.4f} | Recall: {recall:.4f} | ROC-AUC: {roc_auc_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b83f22",
   "metadata": {},
   "source": [
    "## Paso 3.3: Comparaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb704fe6",
   "metadata": {},
   "source": [
    "**Comparaci√≥n entre Random Forest (SMOTE 10%) y XGBoost (baseline)**\n",
    "\n",
    "Random Forest suele beneficiarse de un leve sobremuestreo sint√©tico para aumentar el recall de la clase positiva; SMOTE 10% fue seleccionado porque mejora F1/recall sin provocar sobre‚Äëbalanceo. XGBoost baseline obtuvo la mejor F1 y ROC‚ÄëAUC sin necesidad de SMOTE, lo que sugiere que su arquitectura de boosting maneja mejor la se√±al y el ruido en este problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72bc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Modelo Configuraci√≥n  Accuracy  Precision   Recall  F1-Score  ROC-AUC\n",
      "Random Forest     SMOTE 10%  0.901054   0.733108 0.568063  0.640118 0.917424\n",
      "      XGBoost      Baseline  0.903082   0.721362 0.609948  0.660993 0.928317\n",
      "1. Random Forest (SMOTE 10%)\n",
      "   Mejor modelo tradicional con balanceo SMOTE\n",
      "\n",
      "2. XGBoost (Baseline)\n",
      "   üèÜ Mejor F1-Score general (sin necesidad de SMOTE)\n",
      "\n",
      "XGBoost Baseline supera a RF+SMOTE 10% en:\n",
      "- F1-Score: 0.6610 vs 0.6401 (+3.26%)\n",
      "- Recall: 0.6099 vs 0.5681 (+7.37%)\n",
      "- ROC-AUC: 0.9283 vs 0.9174\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_models = []\n",
    "\n",
    "# 1. Random Forest + SMOTE 10% (Mejor RF)\n",
    "top_models.append({\n",
    "    'Modelo': 'Random Forest',\n",
    "    'Configuraci√≥n': 'SMOTE 10%',\n",
    "    'Accuracy': smote_results['SMOTE 10%']['Random Forest']['Accuracy'],\n",
    "    'Precision': smote_results['SMOTE 10%']['Random Forest']['Precision'],\n",
    "    'Recall': smote_results['SMOTE 10%']['Random Forest']['Recall'],\n",
    "    'F1-Score': smote_results['SMOTE 10%']['Random Forest']['F1-Score'],\n",
    "    'ROC-AUC': smote_results['SMOTE 10%']['Random Forest']['ROC-AUC'],\n",
    "    'Ventaja': 'Mejor modelo tradicional con balanceo SMOTE'\n",
    "})\n",
    "\n",
    "# 2. XGBoost Baseline (Mejor configuraci√≥n general)\n",
    "top_models.append({\n",
    "    'Modelo': 'XGBoost',\n",
    "    'Configuraci√≥n': 'Baseline',\n",
    "    'Accuracy': baseline_results['XGBoost']['Accuracy'],\n",
    "    'Precision': baseline_results['XGBoost']['Precision'],\n",
    "    'Recall': baseline_results['XGBoost']['Recall'],\n",
    "    'F1-Score': baseline_results['XGBoost']['F1-Score'],\n",
    "    'ROC-AUC': baseline_results['XGBoost']['ROC-AUC'],\n",
    "    'Ventaja': 'üèÜ Mejor F1-Score general (sin necesidad de SMOTE)'\n",
    "})\n",
    "\n",
    "df_top_models = pd.DataFrame(top_models)\n",
    "print(df_top_models[['Modelo', 'Configuraci√≥n', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].to_string(index=False))\n",
    "\n",
    "for i, model in enumerate(top_models, 1):\n",
    "    print(f\"{i}. {model['Modelo']} ({model['Configuraci√≥n']})\")\n",
    "    print(f\"   {model['Ventaja']}\")\n",
    "    print()\n",
    "\n",
    "# An√°lisis comparativo\n",
    "print(f\"XGBoost Baseline supera a RF+SMOTE 10% en:\")\n",
    "print(f\"- F1-Score: {baseline_results['XGBoost']['F1-Score']:.4f} vs {smote_results['SMOTE 10%']['Random Forest']['F1-Score']:.4f} (+{((baseline_results['XGBoost']['F1-Score']/smote_results['SMOTE 10%']['Random Forest']['F1-Score'])-1)*100:.2f}%)\")\n",
    "print(f\"- Recall: {baseline_results['XGBoost']['Recall']:.4f} vs {smote_results['SMOTE 10%']['Random Forest']['Recall']:.4f} (+{((baseline_results['XGBoost']['Recall']/smote_results['SMOTE 10%']['Random Forest']['Recall'])-1)*100:.2f}%)\")\n",
    "print(f\"- ROC-AUC: {baseline_results['XGBoost']['ROC-AUC']:.4f} vs {smote_results['SMOTE 10%']['Random Forest']['ROC-AUC']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380317e5",
   "metadata": {},
   "source": [
    "### Resumen FASE 3: SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e81bd5",
   "metadata": {},
   "source": [
    "### Resumen FASE 3: Balanceo de Clases y Selecci√≥n de Modelo\n",
    "\n",
    "#### üìä **Objetivo de la Fase**\n",
    "Aplicar t√©cnicas de balanceo de clases (SMOTE) de forma incremental para mejorar la capacidad de los modelos de detectar la clase minoritaria (compradores), siguiendo la metodolog√≠a del profesor: aplicar SMOTE solo en entrenamiento, no sobre-balancear, y evaluar el efecto progresivo (5%, 10%, 15%).\n",
    "\n",
    "#### üî¨ **Metodolog√≠a Aplicada**\n",
    "\n",
    "1. **Modelos Base Evaluados** (6 algoritmos):\n",
    "   - Logistic Regression, Decision Tree, Random Forest, SVM, KNN, XGBoost\n",
    "   - Todos entrenados con datos desbalanceados originales (84.5% / 15.5%)\n",
    "\n",
    "2. **Aplicaci√≥n de SMOTE**:\n",
    "   - **SMOTE 5%**: Genera 493 muestras sint√©ticas ‚Üí 2,019 minoritarias (19.50%)\n",
    "   - **SMOTE 10%**: Genera 986 muestras sint√©ticas ‚Üí 2,512 minoritarias (23.15%)\n",
    "   - **SMOTE 15%**: Genera 1,479 muestras sint√©ticas ‚Üí 3,005 minoritarias (26.48%)\n",
    "   - ‚úÖ SMOTE aplicado SOLO en conjunto de entrenamiento\n",
    "   - ‚úÖ Conjunto de prueba sin modificar (evaluaci√≥n realista)\n",
    "\n",
    "3. **Comparaci√≥n de T√©cnicas de Balanceo**:\n",
    "   - SMOTE (over-sampling con muestras sint√©ticas)\n",
    "   - scale_pos_weight en XGBoost (penalizaci√≥n de clases)\n",
    "\n",
    "#### üìà **Resultados Principales**\n",
    "\n",
    "**Random Forest con SMOTE:**\n",
    "| Configuraci√≥n | F1-Score | Recall | ROC-AUC | Mejora F1 vs Baseline |\n",
    "|---------------|----------|--------|---------|----------------------|\n",
    "| Baseline      | 0.6059   | 51.31% | 0.9187  | -                    |\n",
    "| SMOTE 5%      | 0.6372   | 56.54% | 0.9166  | +5.16%              |\n",
    "| **SMOTE 10%** | **0.6401** | **56.81%** | **0.9174** | **+5.64%** |\n",
    "| SMOTE 15%     | 0.6304   | 57.59% | 0.9160  | +4.04%              |\n",
    "\n",
    "**XGBoost con diferentes configuraciones:**\n",
    "| Configuraci√≥n | F1-Score | Recall | ROC-AUC | Observaci√≥n |\n",
    "|---------------|----------|--------|---------|-------------|\n",
    "| **Baseline**  | **0.6610** | **60.99%** | **0.9283** | **üèÜ Mejor general** |\n",
    "| SMOTE 5%      | 0.6535   | 60.73% | 0.9302  | No mejora baseline |\n",
    "| SMOTE 10%     | 0.6509   | 61.26% | 0.9301  | No mejora baseline |\n",
    "| SMOTE 15%     | 0.6475   | 61.78% | 0.9279  | No mejora baseline |\n",
    "\n",
    "#### üéØ **Hallazgos Clave**\n",
    "\n",
    "1. **XGBoost supera a Random Forest sin necesidad de balanceo**:\n",
    "   - XGBoost Baseline (F1=0.6610) > RF + SMOTE 10% (F1=0.6401)\n",
    "   - Mejora del +3.27% en F1-Score\n",
    "   - Mejora del +7.35% en Recall\n",
    "\n",
    "2. **SMOTE es efectivo para Random Forest pero no para XGBoost**:\n",
    "   - RF mejora consistentemente con SMOTE hasta 10% (luego empeora por sobre-balanceo)\n",
    "   - XGBoost baseline ya maneja bien el desbalanceo sin SMOTE\n",
    "\n",
    "3. **SMOTE 10% es √≥ptimo para Random Forest**:\n",
    "   - Mejor balance entre mejora de recall y mantenimiento de precisi√≥n\n",
    "   - SMOTE 15% causa sobre-balanceo (F1 disminuye)\n",
    "\n",
    "4. **scale_pos_weight maximiza recall pero compromete precisi√≥n**:\n",
    "4. **XGBoost maneja mejor el desbalanceo que Random Forest**:\n",
    "   - XGBoost baseline (sin SMOTE) supera a RF + SMOTE 10%\n",
    "   - Arquitectura de √°rboles potenciados maneja naturalmente clases desbalanceadas\n",
    "   - No requiere t√©cnicas de balanceo adicionales\n",
    "#### ‚úÖ **Conclusiones y Decisi√≥n**\n",
    "\n",
    "**Modelo seleccionado para optimizaci√≥n: XGBoost Baseline**\n",
    "\n",
    "**Justificaci√≥n:**\n",
    "- ‚úÖ Mejor F1-Score general (0.6610)\n",
    "- ‚úÖ Excelente ROC-AUC (0.9283)\n",
    "- ‚úÖ Buen balance precisi√≥n-recall (72.14% / 60.99%)\n",
    "- ‚úÖ No requiere SMOTE (m√°s simple, menor costo computacional)\n",
    "- ‚úÖ M√°s r√°pido en entrenamiento (0.50s vs Random Forest)\n",
    "\n",
    "**Siguiente paso:** Optimizaci√≥n de hiperpar√°metros con GridSearchCV sobre XGBoost Baseline para mejorar a√∫n m√°s el rendimiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
