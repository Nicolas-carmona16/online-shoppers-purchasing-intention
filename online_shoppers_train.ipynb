{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114461cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ucimlrepo) (2024.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nicolas\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nicolas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ucimlrepo matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358c58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70859f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "online_shoppers_purchasing_intention_dataset = fetch_ucirepo(id=468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44e6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (as pandas dataframes)\n",
    "X = online_shoppers_purchasing_intention_dataset.data.features\n",
    "y = online_shoppers_purchasing_intention_dataset.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef7db54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 468, 'name': 'Online Shoppers Purchasing Intention Dataset', 'repository_url': 'https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset', 'data_url': 'https://archive.ics.uci.edu/static/public/468/data.csv', 'abstract': 'Of the 12,330 sessions in the dataset,\\n84.5% (10,422) were negative class samples that did not\\nend with shopping, and the rest (1908) were positive class\\nsamples ending with shopping.', 'area': 'Business', 'tasks': ['Classification', 'Clustering'], 'characteristics': ['Multivariate'], 'num_instances': 12330, 'num_features': 17, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Revenue'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2018, 'last_updated': 'Thu Jan 11 2024', 'dataset_doi': '10.24432/C5F88Q', 'creators': ['C. Sakar', 'Yomi Kastro'], 'intro_paper': {'ID': 367, 'type': 'NATIVE', 'title': 'Real-time prediction of online shoppers’ purchasing intention using multilayer perceptron and LSTM recurrent neural networks', 'authors': 'C. O. Sakar, S. Polat, Mete Katircioglu, Yomi Kastro', 'venue': 'Neural computing & applications (Print)', 'year': 2019, 'journal': None, 'DOI': '10.1007/s00521-018-3523-0', 'URL': 'https://www.semanticscholar.org/paper/747e098f85ca2d20afd6313b11242c0c427e6fb3', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The dataset consists of feature vectors belonging to 12,330 sessions. \\r\\nThe dataset was formed so that each session\\r\\nwould belong to a different user in a 1-year period to avoid\\r\\nany tendency to a specific campaign, special day, user\\r\\nprofile, or period. ', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The dataset consists of 10 numerical and 8 categorical attributes.\\r\\nThe \\'Revenue\\' attribute can be used as the class label.\\r\\n\\r\\n\"Administrative\", \"Administrative Duration\", \"Informational\", \"Informational Duration\", \"Product Related\" and \"Product Related Duration\" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. The \"Bounce Rate\", \"Exit Rate\" and \"Page Value\" features represent the metrics measured by \"Google Analytics\" for each page in the e-commerce site. The value of \"Bounce Rate\" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session. The value of \"Exit Rate\" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. The \"Page Value\" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. The \"Special Day\" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine\\'s Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. The dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.', 'citation': None}}\n"
     ]
    }
   ],
   "source": [
    "# metadata\n",
    "print(online_shoppers_purchasing_intention_dataset.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360e5ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name     role         type demographic description  \\\n",
      "0            Administrative  Feature      Integer        None        None   \n",
      "1   Administrative_Duration  Feature      Integer        None        None   \n",
      "2             Informational  Feature      Integer        None        None   \n",
      "3    Informational_Duration  Feature      Integer        None        None   \n",
      "4            ProductRelated  Feature      Integer        None        None   \n",
      "5   ProductRelated_Duration  Feature   Continuous        None        None   \n",
      "6               BounceRates  Feature   Continuous        None        None   \n",
      "7                 ExitRates  Feature   Continuous        None        None   \n",
      "8                PageValues  Feature      Integer        None        None   \n",
      "9                SpecialDay  Feature      Integer        None        None   \n",
      "10                    Month  Feature  Categorical        None        None   \n",
      "11         OperatingSystems  Feature      Integer        None        None   \n",
      "12                  Browser  Feature      Integer        None        None   \n",
      "13                   Region  Feature      Integer        None        None   \n",
      "14              TrafficType  Feature      Integer        None        None   \n",
      "15              VisitorType  Feature  Categorical        None        None   \n",
      "16                  Weekend  Feature       Binary        None        None   \n",
      "17                  Revenue   Target       Binary        None        None   \n",
      "\n",
      "   units missing_values  \n",
      "0   None             no  \n",
      "1   None             no  \n",
      "2   None             no  \n",
      "3   None             no  \n",
      "4   None             no  \n",
      "5   None             no  \n",
      "6   None             no  \n",
      "7   None             no  \n",
      "8   None             no  \n",
      "9   None             no  \n",
      "10  None             no  \n",
      "11  None             no  \n",
      "12  None             no  \n",
      "13  None             no  \n",
      "14  None             no  \n",
      "15  None             no  \n",
      "16  None             no  \n",
      "17  None             no  \n"
     ]
    }
   ],
   "source": [
    "# variable information\n",
    "print(online_shoppers_purchasing_intention_dataset.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0c10e",
   "metadata": {},
   "source": [
    "# FASE 1: PREPARACIÓN DE DATOS\n",
    "\n",
    "## Paso 1.1: División Train/Test (Estratificada)\n",
    "\n",
    "Usamos una configuración de 80% entrenamiento y 20% test, de esta forma queda de la siguiente manera: \n",
    "- 9,864 train / 2,466 test\n",
    "- Desbalance 85/15: ~370 casos positivos en test (suficiente para evaluación confiable)\n",
    "- Suficientes datos para entrenar y aplicar SMOTE posteriormente\n",
    "\n",
    "Además, con la estratificación se mantiene la proporción 85/15 en ambos conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd0bc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de los conjuntos:\n",
      "Train: 9,864 muestras (80.0%)\n",
      "Test:  2,466 muestras (20.0%)\n",
      "\n",
      "Distribución de clases en TRAIN:\n",
      "False (No compra): 8,338 (84.53%)\n",
      "True (Compra):     1,526 (15.47%)\n",
      "\n",
      "Distribución de clases en TEST:\n",
      "False (No compra): 2,084 (84.51%)\n",
      "True (Compra):     382 (15.49%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.20,           # Definimos el 20% para test\n",
    "    random_state=42,          # Reproducibilidad\n",
    "    stratify=y                # Mantiene proporción de clases\n",
    ")\n",
    "\n",
    "print(f\"Tamaño de los conjuntos:\")\n",
    "print(f\"Train: {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test:  {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribución de clases en TRAIN:\")\n",
    "train_counts = y_train['Revenue'].value_counts()\n",
    "print(f\"False (No compra): {train_counts[False]:,} ({train_counts[False]/len(y_train)*100:.2f}%)\")\n",
    "print(f\"True (Compra):     {train_counts[True]:,} ({train_counts[True]/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDistribución de clases en TEST:\")\n",
    "test_counts = y_test['Revenue'].value_counts()\n",
    "print(f\"False (No compra): {test_counts[False]:,} ({test_counts[False]/len(y_test)*100:.2f}%)\")\n",
    "print(f\"True (Compra):     {test_counts[True]:,} ({test_counts[True]/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2faa12",
   "metadata": {},
   "source": [
    "## Paso 1.2: Codificación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2a52ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación completada\n",
      "X_train: (9864, 43)\n",
      "X_test:  (2466, 43)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "# 1. WEEKEND: Bool → Int\n",
    "X_train_encoded['Weekend'] = X_train_encoded['Weekend'].astype(int)\n",
    "X_test_encoded['Weekend'] = X_test_encoded['Weekend'].astype(int)\n",
    "\n",
    "# 2. MONTH: OneHot\n",
    "month_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "month_encoded_train = month_encoder.fit_transform(X_train_encoded[['Month']])\n",
    "month_encoded_test = month_encoder.transform(X_test_encoded[['Month']])\n",
    "month_cols = [f'Month_{cat}' for cat in month_encoder.categories_[0][1:]]\n",
    "month_train_df = pd.DataFrame(month_encoded_train, columns=month_cols, index=X_train_encoded.index)\n",
    "month_test_df = pd.DataFrame(month_encoded_test, columns=month_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('Month', axis=1), month_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('Month', axis=1), month_test_df], axis=1)\n",
    "\n",
    "# 3. VISITORTYPE: OneHot\n",
    "visitor_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "visitor_encoded_train = visitor_encoder.fit_transform(X_train_encoded[['VisitorType']])\n",
    "visitor_encoded_test = visitor_encoder.transform(X_test_encoded[['VisitorType']])\n",
    "visitor_cols = [f'VisitorType_{cat}' for cat in visitor_encoder.categories_[0][1:]]\n",
    "visitor_train_df = pd.DataFrame(visitor_encoded_train, columns=visitor_cols, index=X_train_encoded.index)\n",
    "visitor_test_df = pd.DataFrame(visitor_encoded_test, columns=visitor_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('VisitorType', axis=1), visitor_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('VisitorType', axis=1), visitor_test_df], axis=1)\n",
    "\n",
    "# 4. OPERATINGSYSTEMS: OneHot\n",
    "os_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "os_encoded_train = os_encoder.fit_transform(X_train_encoded[['OperatingSystems']])\n",
    "os_encoded_test = os_encoder.transform(X_test_encoded[['OperatingSystems']])\n",
    "os_cols = [f'OS_{int(cat)}' for cat in os_encoder.categories_[0][1:]]\n",
    "os_train_df = pd.DataFrame(os_encoded_train, columns=os_cols, index=X_train_encoded.index)\n",
    "os_test_df = pd.DataFrame(os_encoded_test, columns=os_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('OperatingSystems', axis=1), os_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('OperatingSystems', axis=1), os_test_df], axis=1)\n",
    "\n",
    "# 5. BROWSER: OneHot con Grouping (Top 5 + Other)\n",
    "top_5_browsers = X_train_encoded['Browser'].value_counts().head(5).index.tolist()\n",
    "X_train_encoded['Browser_grouped'] = X_train_encoded['Browser'].apply(\n",
    "    lambda x: x if x in top_5_browsers else 99\n",
    ")\n",
    "X_test_encoded['Browser_grouped'] = X_test_encoded['Browser'].apply(\n",
    "    lambda x: x if x in top_5_browsers else 99\n",
    ")\n",
    "browser_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "browser_encoded_train = browser_encoder.fit_transform(X_train_encoded[['Browser_grouped']])\n",
    "browser_encoded_test = browser_encoder.transform(X_test_encoded[['Browser_grouped']])\n",
    "browser_cols = [f'Browser_{int(cat) if cat != 99 else \"Other\"}' for cat in browser_encoder.categories_[0][1:]]\n",
    "browser_train_df = pd.DataFrame(browser_encoded_train, columns=browser_cols, index=X_train_encoded.index)\n",
    "browser_test_df = pd.DataFrame(browser_encoded_test, columns=browser_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop(['Browser', 'Browser_grouped'], axis=1), browser_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop(['Browser', 'Browser_grouped'], axis=1), browser_test_df], axis=1)\n",
    "\n",
    "# 6. REGION: OneHot\n",
    "region_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "region_encoded_train = region_encoder.fit_transform(X_train_encoded[['Region']])\n",
    "region_encoded_test = region_encoder.transform(X_test_encoded[['Region']])\n",
    "region_cols = [f'Region_{int(cat)}' for cat in region_encoder.categories_[0][1:]]\n",
    "region_train_df = pd.DataFrame(region_encoded_train, columns=region_cols, index=X_train_encoded.index)\n",
    "region_test_df = pd.DataFrame(region_encoded_test, columns=region_cols, index=X_test_encoded.index)\n",
    "X_train_encoded = pd.concat([X_train_encoded.drop('Region', axis=1), region_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test_encoded.drop('Region', axis=1), region_test_df], axis=1)\n",
    "\n",
    "# 7. TRAFFICTYPE: Target Encoding\n",
    "traffic_conversion_rate = X_train_encoded.join(y_train).groupby('TrafficType')['Revenue'].mean().to_dict()\n",
    "global_mean = y_train['Revenue'].mean()\n",
    "X_train_encoded['TrafficType_Encoded'] = X_train_encoded['TrafficType'].map(traffic_conversion_rate)\n",
    "X_test_encoded['TrafficType_Encoded'] = X_test_encoded['TrafficType'].map(traffic_conversion_rate).fillna(global_mean)\n",
    "X_train_encoded = X_train_encoded.drop('TrafficType', axis=1)\n",
    "X_test_encoded = X_test_encoded.drop('TrafficType', axis=1)\n",
    "\n",
    "print(f\"Codificación completada\")\n",
    "print(f\"X_train: {X_train_encoded.shape}\")\n",
    "print(f\"X_test:  {X_test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46df8c",
   "metadata": {},
   "source": [
    "### Resumen de Codificación Completada\n",
    "\n",
    "**Transformaciones aplicadas:**\n",
    "\n",
    "| Variable Original | Estrategia | Columnas Generadas | Justificación |\n",
    "|------------------|------------|-------------------|---------------|\n",
    "| **Month** | OneHot (drop first) | 9 | Sin orden natural, captura estacionalidad |\n",
    "| **VisitorType** | OneHot (drop first) | 2 | Solo 3 categorías nominales |\n",
    "| **Weekend** | Bool → Int | 1 | Ya binaria, solo conversión |\n",
    "| **OperatingSystems** | OneHot (drop first) | 7 | 8 valores manejables |\n",
    "| **Browser** | OneHot + Grouping | 5 | Top 5 + \"Other\" (reducido de 13) |\n",
    "| **Region** | OneHot (drop first) | 8 | 9 valores geográficos |\n",
    "| **TrafficType** | Target Encoding | 1 | 20 valores → 1 numérica |\n",
    "\n",
    "**Resultado:**\n",
    "- Features originales: 17\n",
    "- Features después de codificación: **43** (vs 73 con OneHot completo)\n",
    "- Reducción de dimensionalidad: 41% menos features\n",
    "- Todos los encoders ajustados SOLO con train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1795d8",
   "metadata": {},
   "source": [
    "## Paso 1.3: Escalado de Variables Numéricas\n",
    "\n",
    "Usaremos **RobustScaler** porque en el análisis exploratorio se vieron muchos outliers en variables de duración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c78b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled: (9864, 43)\n",
      "X_test_scaled:  (2466, 43)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "numerical_cols_to_scale = [\n",
    "    'Administrative', 'Administrative_Duration',\n",
    "    'Informational', 'Informational_Duration',\n",
    "    'ProductRelated', 'ProductRelated_Duration',\n",
    "    'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay',\n",
    "    'TrafficType_Encoded'\n",
    "]\n",
    "\n",
    "X_train_scaled = X_train_encoded.copy()\n",
    "X_test_scaled = X_test_encoded.copy()\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_train_encoded[numerical_cols_to_scale])\n",
    "X_test_scaled[numerical_cols_to_scale] = scaler.transform(X_test_encoded[numerical_cols_to_scale])\n",
    "\n",
    "print(f\"X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled:  {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538985e6",
   "metadata": {},
   "source": [
    "### Resumen de la fase 1: Preparación de Datos\n",
    "\n",
    "| Paso | Acción |\n",
    "|------|--------|\n",
    "| 1.1 | División Train/Test (80/20 estratificado) |\n",
    "| 1.2 | Codificación de variables categóricas |\n",
    "| 1.3 | Escalado de variables numéricas (RobustScaler) |\n",
    "\n",
    "**Datasets listos para entrenamiento:**\n",
    "- `X_train_scaled`: 9,864 muestras × 43 features\n",
    "- `X_test_scaled`: 2,466 muestras × 43 features\n",
    "- `y_train`: 9,864 etiquetas (84.5% No compra, 15.5% Compra)\n",
    "- `y_test`: 2,466 etiquetas (84.5% No compra, 15.5% Compra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb98ea0d",
   "metadata": {},
   "source": [
    "# FASE 2: MODELO BASELINE\n",
    "\n",
    "## Paso 2.1: Entrenamiento de Modelos sin Balanceo\n",
    "\n",
    "Entrenamos varios modelos con los datos desbalanceados (84.5% No compra / 15.5% Compra) para establecer una línea base, esto tambiem nos permitirá comparar el efecto de SMOTE posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c1afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Aplanar y_train y y_test para compatibilidad\n",
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "baseline_results = {}\n",
    "\n",
    "# Modelos a entrenar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=2000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train_scaled, y_train_flat)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "    precision = precision_score(y_test_flat, y_pred)\n",
    "    recall = recall_score(y_test_flat, y_pred)\n",
    "    f1 = f1_score(y_test_flat, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test_flat, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Resultados\n",
    "    baseline_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Model': model\n",
    "    }\n",
    "    \n",
    "    roc_auc_str = f\"{roc_auc:.4f}\" if roc_auc is not None else \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e34381",
   "metadata": {},
   "source": [
    "## Paso 2.2: Evaluación y Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d288b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy Precision    Recall  F1-Score   ROC-AUC Training Time (s)\n",
      "Logistic Regression  0.882401  0.755556  0.356021  0.483986  0.886398           1.51645\n",
      "Decision Tree        0.856853  0.537662  0.541885  0.539765  0.728236          0.085332\n",
      "Random Forest        0.896594  0.739623  0.513089  0.605873  0.918734            1.2663\n",
      "SVM                   0.88159  0.692308  0.424084  0.525974  0.851294          8.077825\n",
      "KNN                  0.886456  0.675862  0.513089  0.583333  0.848243           0.38414\n",
      "\n",
      "Mejor modelo (por F1-Score): Random Forest\n",
      "F1-Score: 0.6059\n",
      "ROC-AUC: 0.9187\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(baseline_results).T\n",
    "results_df = results_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Training Time (s)']]\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Identificar mejor modelo por F1-Score (ya que es más apropiado para datos desbalanceados)\n",
    "best_model_name = results_df['F1-Score'].idxmax()\n",
    "print(f\"\\nMejor modelo (por F1-Score): {best_model_name}\")\n",
    "print(f\"F1-Score: {results_df.loc[best_model_name, 'F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {results_df.loc[best_model_name, 'ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73ad744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de Random Forest\n",
      "\n",
      "Matriz de Confusión:\n",
      "                  Predicho: No Compra | Predicho: Compra\n",
      "Real: No Compra            2015     |         69\n",
      "Real: Compra                186     |        196\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Compra       0.92      0.97      0.94      2084\n",
      "      Compra       0.74      0.51      0.61       382\n",
      "\n",
      "    accuracy                           0.90      2466\n",
      "   macro avg       0.83      0.74      0.77      2466\n",
      "weighted avg       0.89      0.90      0.89      2466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matriz de confusión y reporte detallado del mejor modelo\n",
    "best_model = baseline_results[best_model_name]['Model']\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Análisis de {best_model_name}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test_flat, y_pred_best)\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(f\"                  Predicho: No Compra | Predicho: Compra\")\n",
    "print(f\"Real: No Compra          {cm[0][0]:6d}     |     {cm[0][1]:6d}\")\n",
    "print(f\"Real: Compra             {cm[1][0]:6d}     |     {cm[1][1]:6d}\")\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test_flat, y_pred_best, target_names=['No Compra', 'Compra']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f942d1",
   "metadata": {},
   "source": [
    "### Resumen FASE 2: Modelo Baseline\n",
    "\n",
    "**Resultados obtenidos Sin balanceo de clases:**\n",
    "\n",
    "| Modelo | Accuracy | Precision | Recall | F1-Score | ROC-AUC |\n",
    "|--------|----------|-----------|--------|----------|---------|\n",
    "| **Random Forest** | **0.8966** | **0.7396** | **0.5131** | **0.6059** | **0.9187** |\n",
    "| KNN | 0.8865 | 0.6759 | 0.5131 | 0.5833 | 0.8482 |\n",
    "| Logistic Regression | 0.8824 | 0.7556 | 0.3560 | 0.4840 | 0.8862 |\n",
    "| SVM | 0.8816 | 0.6923 | 0.4241 | 0.5260 | 0.8513 |\n",
    "| Decision Tree | 0.8569 | 0.5377 | 0.5419 | 0.5398 | 0.7282 |\n",
    "\n",
    "**Análisis:**\n",
    "\n",
    "1. Random Forest es el mejor modelo con F1=0.6059 y ROC-AUC=0.9187\n",
    "2. Problema del desbalanceo es evidente:\n",
    "   - Alta accuracy (89.7%) pero bajo recall (51.3%) para clase positiva\n",
    "   - El modelo predice bien \"No Compra\" (97% recall) pero falla en \"Compra\" (51% recall)\n",
    "   - 186 falsos negativos (casi la mitad de las compras no detectadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc2f55",
   "metadata": {},
   "source": [
    "# FASE 3: SMOTE PROGRESIVO\n",
    "\n",
    "## Paso 3.1: Aplicación de SMOTE Incremental\n",
    "\n",
    "Se aplica SMOTE incrementalmente generando 5%, 10% y 15% adicional de muestras de la clase minoritaria. SMOTE se aplica SOLO en el conjunto de entrenamiento.\n",
    "\n",
    "**Estado actual:**\n",
    "- Clase minoritaria (Compra): 1,526 muestras (15.47%)\n",
    "- Clase mayoritaria (No Compra): 8,338 muestras (84.53%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ccd11fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalar imbalanced-learn si no está instalado\n",
    "%pip install imbalanced-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad9c1d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMOTE 5%:\n",
      "- Muestras minoritarias: 2019 (19.49%)\n",
      "- sampling_strategy: 0.2421\n",
      "\n",
      " SMOTE 10%:\n",
      "- Muestras minoritarias: 2512 (23.15%)\n",
      "- sampling_strategy: 0.3013\n",
      "\n",
      "SMOTE 15%:\n",
      "- Muestras minoritarias: 3005 (26.49%)\n",
      "- sampling_strategy: 0.3604\n",
      "\n",
      "SMOTE 5%:\n",
      "Total muestras: 10357\n",
      "Compra (1): 2019 (19.49%)\n",
      "No Compra (0): 8338 (80.51%)\n",
      "\n",
      "SMOTE 10%:\n",
      "Total muestras: 10850\n",
      "Compra (1): 2512 (23.15%)\n",
      "No Compra (0): 8338 (76.85%)\n",
      "\n",
      "SMOTE 15%:\n",
      "Total muestras: 11343\n",
      "Compra (1): 3005 (26.49%)\n",
      "No Compra (0): 8338 (73.51%)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Estado actual de las clases en train\n",
    "current_minority = (y_train_flat == 1).sum()  # 1526\n",
    "current_majority = (y_train_flat == 0).sum()  # 8338\n",
    "total_train = len(y_train_flat)\n",
    "\n",
    "# Hacemos el Calculo sampling_strategy para cada nivel de SMOTE\n",
    "# sampling_strategy = num_samples_minority / num_samples_majority\n",
    "\n",
    "# SMOTE 5%\n",
    "smote_5_samples = current_minority + int(total_train * 0.05)\n",
    "strategy_5 = smote_5_samples / current_majority\n",
    "\n",
    "# SMOTE 10%\n",
    "smote_10_samples = current_minority + int(total_train * 0.10)\n",
    "strategy_10 = smote_10_samples / current_majority\n",
    "\n",
    "# SMOTE 15%\n",
    "smote_15_samples = current_minority + int(total_train * 0.15)\n",
    "strategy_15 = smote_15_samples / current_majority\n",
    "\n",
    "print(f\"\\nSMOTE 5%:\")\n",
    "print(f\"- Muestras minoritarias: {smote_5_samples} ({smote_5_samples/(current_majority+smote_5_samples)*100:.2f}%)\")\n",
    "print(f\"- sampling_strategy: {strategy_5:.4f}\")\n",
    "\n",
    "print(f\"\\n SMOTE 10%:\")\n",
    "print(f\"- Muestras minoritarias: {smote_10_samples} ({smote_10_samples/(current_majority+smote_10_samples)*100:.2f}%)\")\n",
    "print(f\"- sampling_strategy: {strategy_10:.4f}\")\n",
    "\n",
    "print(f\"\\nSMOTE 15%:\")\n",
    "print(f\"- Muestras minoritarias: {smote_15_samples} ({smote_15_samples/(current_majority+smote_15_samples)*100:.2f}%)\")\n",
    "print(f\"- sampling_strategy: {strategy_15:.4f}\")\n",
    "\n",
    "# Se aplica SMOTE para cada nivel\n",
    "smote_configs = {\n",
    "    'SMOTE 5%': (strategy_5, smote_5_samples),\n",
    "    'SMOTE 10%': (strategy_10, smote_10_samples),\n",
    "    'SMOTE 15%': (strategy_15, smote_15_samples)\n",
    "}\n",
    "\n",
    "smote_datasets = {}\n",
    "\n",
    "for name, (strategy, expected_samples) in smote_configs.items():\n",
    "    smote = SMOTE(sampling_strategy=strategy, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train_flat)\n",
    "    \n",
    "    smote_datasets[name] = {\n",
    "        'X': X_resampled,\n",
    "        'y': y_resampled,\n",
    "        'minority_count': (y_resampled == 1).sum(),\n",
    "        'majority_count': (y_resampled == 0).sum(),\n",
    "        'total': len(y_resampled)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Total muestras: {smote_datasets[name]['total']}\")\n",
    "    print(f\"Compra (1): {smote_datasets[name]['minority_count']} ({smote_datasets[name]['minority_count']/smote_datasets[name]['total']*100:.2f}%)\")\n",
    "    print(f\"No Compra (0): {smote_datasets[name]['majority_count']} ({smote_datasets[name]['majority_count']/smote_datasets[name]['total']*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bea9dd",
   "metadata": {},
   "source": [
    "## Paso 3.2: Entrenamiento con SMOTE y Evaluación\n",
    "\n",
    "Entrenaremos los 5 modelos con cada configuración de SMOTE y compararemos con el baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1028f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 5%\n",
      "  Logistic Regression  | F1: 0.5240 | Recall: 0.4136 | ROC-AUC: 0.8876\n",
      "  Decision Tree        | F1: 0.5181 | Recall: 0.5236 | ROC-AUC: 0.7162\n",
      "  Logistic Regression  | F1: 0.5240 | Recall: 0.4136 | ROC-AUC: 0.8876\n",
      "  Decision Tree        | F1: 0.5181 | Recall: 0.5236 | ROC-AUC: 0.7162\n",
      "  Random Forest        | F1: 0.6372 | Recall: 0.5654 | ROC-AUC: 0.9166\n",
      "  Random Forest        | F1: 0.6372 | Recall: 0.5654 | ROC-AUC: 0.9166\n",
      "  SVM                  | F1: 0.5644 | Recall: 0.4817 | ROC-AUC: 0.8688\n",
      "  KNN                  | F1: 0.5863 | Recall: 0.5733 | ROC-AUC: 0.8437\n",
      "SMOTE 10%\n",
      "  SVM                  | F1: 0.5644 | Recall: 0.4817 | ROC-AUC: 0.8688\n",
      "  KNN                  | F1: 0.5863 | Recall: 0.5733 | ROC-AUC: 0.8437\n",
      "SMOTE 10%\n",
      "  Logistic Regression  | F1: 0.5639 | Recall: 0.4738 | ROC-AUC: 0.8878\n",
      "  Decision Tree        | F1: 0.5160 | Recall: 0.5288 | ROC-AUC: 0.7167\n",
      "  Logistic Regression  | F1: 0.5639 | Recall: 0.4738 | ROC-AUC: 0.8878\n",
      "  Decision Tree        | F1: 0.5160 | Recall: 0.5288 | ROC-AUC: 0.7167\n",
      "  Random Forest        | F1: 0.6401 | Recall: 0.5681 | ROC-AUC: 0.9174\n",
      "  Random Forest        | F1: 0.6401 | Recall: 0.5681 | ROC-AUC: 0.9174\n",
      "  SVM                  | F1: 0.5865 | Recall: 0.5236 | ROC-AUC: 0.8672\n",
      "  KNN                  | F1: 0.5714 | Recall: 0.6178 | ROC-AUC: 0.8437\n",
      "SMOTE 15%\n",
      "  SVM                  | F1: 0.5865 | Recall: 0.5236 | ROC-AUC: 0.8672\n",
      "  KNN                  | F1: 0.5714 | Recall: 0.6178 | ROC-AUC: 0.8437\n",
      "SMOTE 15%\n",
      "  Logistic Regression  | F1: 0.5979 | Recall: 0.5236 | ROC-AUC: 0.8869\n",
      "  Decision Tree        | F1: 0.5768 | Recall: 0.6047 | ROC-AUC: 0.7573\n",
      "  Logistic Regression  | F1: 0.5979 | Recall: 0.5236 | ROC-AUC: 0.8869\n",
      "  Decision Tree        | F1: 0.5768 | Recall: 0.6047 | ROC-AUC: 0.7573\n",
      "  Random Forest        | F1: 0.6304 | Recall: 0.5759 | ROC-AUC: 0.9160\n",
      "  Random Forest        | F1: 0.6304 | Recall: 0.5759 | ROC-AUC: 0.9160\n",
      "  SVM                  | F1: 0.5953 | Recall: 0.5602 | ROC-AUC: 0.8488\n",
      "  KNN                  | F1: 0.5889 | Recall: 0.6806 | ROC-AUC: 0.8511\n",
      "  SVM                  | F1: 0.5953 | Recall: 0.5602 | ROC-AUC: 0.8488\n",
      "  KNN                  | F1: 0.5889 | Recall: 0.6806 | ROC-AUC: 0.8511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import time\n",
    "\n",
    "smote_results = {}\n",
    "\n",
    "models_smote = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=2000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Entrenamos con cada configuración de SMOTE\n",
    "for smote_name, smote_data in smote_datasets.items():\n",
    "    print(f\"{smote_name}\")\n",
    "    \n",
    "    X_smote = smote_data['X']\n",
    "    y_smote = smote_data['y']\n",
    "    \n",
    "    smote_results[smote_name] = {}\n",
    "    \n",
    "    for model_name, model in models_smote.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entrenar\n",
    "        model.fit(X_smote, y_smote)\n",
    "        \n",
    "        # Predecir en test (Recordar que el test es sin SMOTE)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Métricas\n",
    "        accuracy = accuracy_score(y_test_flat, y_pred)\n",
    "        precision = precision_score(y_test_flat, y_pred)\n",
    "        recall = recall_score(y_test_flat, y_pred)\n",
    "        f1 = f1_score(y_test_flat, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test_flat, y_pred_proba) if y_pred_proba is not None else None\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        smote_results[smote_name][model_name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'Training Time (s)': training_time\n",
    "        }\n",
    "        \n",
    "        roc_auc_str = f\"{roc_auc:.4f}\" if roc_auc is not None else \"N/A\"\n",
    "        print(f\"  {model_name:20s} | F1: {f1:.4f} | Recall: {recall:.4f} | ROC-AUC: {roc_auc_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b83f22",
   "metadata": {},
   "source": [
    "## Paso 3.3: Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47804a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Configuración  Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
      "Baseline (Sin SMOTE)    0.8966     0.7396  0.5131    0.6059   0.9187\n",
      "            SMOTE 5%    0.9002     0.7297  0.5654    0.6372   0.9166\n",
      "           SMOTE 10%    0.9011     0.7331  0.5681    0.6401   0.9174\n",
      "           SMOTE 15%    0.8954     0.6962  0.5759    0.6304   0.9160\n",
      "\n",
      "Mejor configuración: SMOTE 10%\n",
      "F1-Score: 0.6401\n",
      "Recall: 0.5681\n",
      "\n",
      "Mejora vs Baseline:\n",
      "F1-Score: +5.64%\n",
      "Recall: +10.72%\n"
     ]
    }
   ],
   "source": [
    "# Comparar Random Forest (mejor modelo baseline) en diferentes configuraciones\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Baseline\n",
    "comparison_data.append({\n",
    "    'Configuración': 'Baseline (Sin SMOTE)',\n",
    "    'Accuracy': baseline_results['Random Forest']['Accuracy'],\n",
    "    'Precision': baseline_results['Random Forest']['Precision'],\n",
    "    'Recall': baseline_results['Random Forest']['Recall'],\n",
    "    'F1-Score': baseline_results['Random Forest']['F1-Score'],\n",
    "    'ROC-AUC': baseline_results['Random Forest']['ROC-AUC']\n",
    "})\n",
    "\n",
    "# SMOTE 5%, 10%, 15%\n",
    "for smote_name in ['SMOTE 5%', 'SMOTE 10%', 'SMOTE 15%']:\n",
    "    comparison_data.append({\n",
    "        'Configuración': smote_name,\n",
    "        'Accuracy': smote_results[smote_name]['Random Forest']['Accuracy'],\n",
    "        'Precision': smote_results[smote_name]['Random Forest']['Precision'],\n",
    "        'Recall': smote_results[smote_name]['Random Forest']['Recall'],\n",
    "        'F1-Score': smote_results[smote_name]['Random Forest']['F1-Score'],\n",
    "        'ROC-AUC': smote_results[smote_name]['Random Forest']['ROC-AUC']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identificar mejor configuración\n",
    "best_f1_idx = comparison_df['F1-Score'].idxmax()\n",
    "best_config = comparison_df.loc[best_f1_idx, 'Configuración']\n",
    "best_f1 = comparison_df.loc[best_f1_idx, 'F1-Score']\n",
    "best_recall = comparison_df.loc[best_f1_idx, 'Recall']\n",
    "\n",
    "print(f\"\\nMejor configuración: {best_config}\")\n",
    "print(f\"F1-Score: {best_f1:.4f}\")\n",
    "print(f\"Recall: {best_recall:.4f}\")\n",
    "\n",
    "# Calcular mejora vs baseline\n",
    "baseline_f1 = comparison_df.loc[0, 'F1-Score']\n",
    "baseline_recall = comparison_df.loc[0, 'Recall']\n",
    "improvement_f1 = ((best_f1 - baseline_f1) / baseline_f1) * 100\n",
    "improvement_recall = ((best_recall - baseline_recall) / baseline_recall) * 100\n",
    "\n",
    "print(f\"\\nMejora vs Baseline:\")\n",
    "print(f\"F1-Score: +{improvement_f1:.2f}%\")\n",
    "print(f\"Recall: +{improvement_recall:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380317e5",
   "metadata": {},
   "source": [
    "### Resumen FASE 3: SMOTE\n",
    "\n",
    "**Resultados de Random Forest con diferentes niveles de SMOTE:**\n",
    "\n",
    "| Configuración | Accuracy | Precision | Recall | F1-Score | ROC-AUC | Mejora F1 | Mejora Recall |\n",
    "|---------------|----------|-----------|--------|----------|---------|-----------|---------------|\n",
    "| **Baseline** | 0.8966 | 0.7396 | 0.5131 | **0.6059** | 0.9187 | - | - |\n",
    "| **SMOTE 5%** | 0.9002 | 0.7297 | 0.5654 | 0.6372 | 0.9166 | +5.16% | +10.19% |\n",
    "| **SMOTE 10%** | **0.9011** | **0.7331** | **0.5681** | **0.6401** | **0.9174** | **+5.64%** | **+10.72%** |\n",
    "| **SMOTE 15%** | 0.8954 | 0.6962 | 0.5759 | 0.6304 | 0.9160 | +4.04% | +12.24% |\n",
    "\n",
    "**Análisis:**\n",
    "\n",
    "1. **SMOTE 10% es la mejor configuración:**\n",
    "   - F1-Score mejoró 5.64% (0.6059 → 0.6401)\n",
    "   - Recall mejoró 10.72% (51.31% → 56.81%)\n",
    "   - Mantiene buen balance entre precision y recall\n",
    "\n",
    "2. **Observaciones:**\n",
    "   - SMOTE 5% mejora solo un poco los resultados\n",
    "   - SMOTE 10% logra el mejor balance\n",
    "   - SMOTE 15% mejora recall pero baja precision (overfitting leve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
